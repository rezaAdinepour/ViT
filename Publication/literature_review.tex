\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}




\title{Literature Review for Ingur Thesis References}
\author{Reza}
\date{\today}

\begin{document}
	
	\maketitle
	\tableofcontents
		
	\chapter{BinaryViT \cite{le2023binaryvit}}
	
	\section{Introduction}
	The paper addresses the challenge of improving the performance of binary Vision Transformers (ViTs), a class of deep learning models used in computer vision. While ViTs have shown great potential, particularly when trained on large datasets, they suffer significant performance loss when binarized — a technique that reduces computational costs by converting model weights and activations into binary values. This performance drop is especially notable compared to convolutional neural networks (CNNs), which handle binarization more effectively.
	
	
	\section{Proposed Model}
	The paper identifies that the architecture of standard ViTs lacks key features present in CNNs, which allows CNNs to maintain higher representational capability even after binarization. To address this, the authors propose BinaryViT, a model that incorporates several features inspired by CNNs into the ViT architecture, without using convolutions. These enhancements include:
	
	\subsection{Global Average Pooling Layer}
	Replacing the token pooling layer with a global average pooling layer, which helps gather more information from input patches.
	
	\textbf{Previous Approach:} In standard ViTs, a token pooling layer is used before the classifier layer, which only takes into account information from the CLS token rather than considering all tokens in the input sequence.
	
	\textbf{Proposed Change:} The authors replace the token pooling layer with a global average pooling layer. This ensures that the model incorporates information from all input tokens (or patches), not just the CLS token. By doing so, the final classifier layer has more flexibility and can capture richer feature representations. This addition significantly increases the representational capability of the binary model by aggregating the information from all patches, which is crucial for improving accuracy in binary settings.
	
	
	\subsection{Multiple Pooling Branches}
	Introducing multiple pooling branches in each block to increase representational capability.
	Adding an affine transformation before each residual connection to balance the scales of different layers.
	
	\textbf{Previous Approach:} Traditional ViTs, and earlier works in binary ViTs, use simple feed-forward layers (FFNs) after attention layers with limited flexibility in processing features.
	
	
	\textbf{Proposed Change:} Inspired by CNNs, where convolutional layers capture different spatial information, the authors introduce multiple average pooling branches in each block. Each branch has different kernel sizes (e.g., 1x3, 3x1, 1x5, 5x1), allowing the model to process and aggregate spatial information in multiple directions. This change enhances the binary ViT’s ability to represent more complex information, without adding significant computational overhead.
	
	
	
	\subsection{Affine Transformation Before Residual Connections}
	Incorporating a pyramid structure to process high-resolution features early on and reduce them as the model goes deeper, increasing its flexibility and performance.
	
	
	\textbf{Previous Approach:} In ViTs, the scale of hidden states grows deeper in the network layers, often causing the residual branches to overwhelm the main branches, leading to a decrease in the model’s effectiveness. Binary CNNs, such as ResNet, use batch normalization before residual connections, which helps balance the scale of different layers and improves performance.
	
	
	\textbf{Proposed Change:} To counter the issue of overwhelming residual connections, the authors introduce an affine transformation before each residual addition in the ViT architecture. This technique is inspired by batch normalization in CNNs, which helps maintain a balance between the main and residual branches. The transformation prevents residual connections from dominating the main branches and allows the binary ViT to maintain better feature flow and representation through deeper layers.
	
	
	
	
	
	
	\subsection{Pyramid Structure}
	\textbf{Previous Approach:} Binary ViTs (like DeiT) typically use a fixed resolution for the feature maps throughout the network, unlike CNNs that progressively downsample the feature maps and increase the number of channels as the network goes deeper. In CNNs, this pyramid structure is important for capturing features at different resolutions and improving representational capacity.
	
	\textbf{Proposed Change:} The authors introduce a pyramid structure in the binary ViT. In this architecture, the feature map size progressively decreases (downsampling) while the hidden dimension (number of channels) increases as the network goes deeper. This mirrors the pyramid structure found in CNNs, allowing the model to capture features at high resolution in the early stages and focus on more abstract, lower-resolution features in the later stages. This significantly improves the model’s ability to handle complex visual tasks, especially when binarized.
	
	
	
	\subsection{Binary Fully-Connected Layers with Enhanced Attention}
	\textbf{Previous Approach:} Standard ViTs rely on attention mechanisms, where matrix multiplications for query, key, and value operations are computationally expensive and prone to significant performance drops when binarized.
	
	\textbf{Proposed Change:} In the proposed BinaryViT model, the authors optimize the binary attention mechanism by modifying how attention probabilities are calculated. They apply scaling factors and rounding techniques to improve the binary attention probability matrix’s accuracy, using methods inspired by prior works like ReActNet and Bi-RealNet in binary CNNs. This enhancement ensures that the binary ViT can more effectively process information during self-attention, resulting in better performance.
	
	
	
	\subsection{Distillation from Full-Precision Models}
	\textbf{Previous Approach:} Previous methods for binary ViTs did not consistently use teacher-student knowledge distillation methods to reduce the performance gap between binary and full-precision models.
	
	\textbf{Proposed Change:} The authors use a full-precision ViT model as a teacher to guide the training of the binary ViT. They distill knowledge by minimizing the soft cross-entropy loss between the binary student model’s logits and the full-precision teacher’s logits. While distillation techniques were used in some prior works, the authors tailor it specifically to improve binary ViT performance, focusing on logits rather than other components like attention scores or feed-forward outputs, which caused performance degradation in previous experiments.
	
	
	\section{Impact of the Changes}
	These architectural modifications collectively improve the performance of binary ViTs, making them competitive with binary CNNs. The proposed BinaryViT model achieves a significant performance boost on the ImageNet-1k dataset, outperforming earlier binary transformer models. By integrating CNN-inspired architectural features into ViTs, the authors have managed to retain the benefits of transformer models while reducing the computational cost and maintaining high accuracy in a binarized setting.
	
	These changes provide a more efficient and flexible architecture for tasks requiring high performance on resource-constrained devices such as smartphones and edge devices.
	
	
	
	\section{Results and Improvements}
	The results and improvements announced in the BinaryViT method, as detailed in the paper, demonstrate significant advancements in the performance of binary Vision Transformers (ViTs) compared to previous approaches. Below is a breakdown of the results and the improvements achieved by this method:
	
	\subsection{Performance Improvement on ImageNet-1k}
	The proposed BinaryViT was evaluated on the ImageNet-1k dataset, a standard benchmark for image classification. The model showed significant performance improvements over baseline binary ViTs and previous state-of-the-art (SOTA) binary models. The key results include:
	\begin{enumerate}
		\item
		\textbf{Baseline binary DeiT-S (previous work):} 48.5\% top-1 accuracy on ImageNet-1k.
		
		
		\item 
		\textbf{BinaryViT (proposed method):} Achieved \textbf{67.7\% top-1 accuracy} using the proposed enhancements, representing a large leap of \textbf{19.2\% improvement} over the baseline binary ViT (DeiT-S).
		
		\item 
		The modified BinaryViT architecture with full-precision patch embedding layers (BinaryViT*) achieved an even higher \textbf{70.6\% top-1 accuracy}, making it competitive with top binary CNNs like ReActNet.
	\end{enumerate}
	
	\subsection{Efficiency in Terms of Operations and Parameters}
	BinaryViT not only improves accuracy but also maintains computational efficiency, making it suitable for deployment on edge devices with limited resources. Key findings include:
	
	\begin{enumerate}
		\item 
		\textbf{Operations (OPs):} The proposed BinaryViT model performed fewer operations compared to many SOTA binary models. For example, BinaryViT had $0.79 \times 10^8$ operations compared to ReActNet’s $1.93 \times 10^8$ operations, making it nearly \textbf{2.5× more efficient}.
		
		\item 
		\textbf{Parameters:} BinaryViT contains around 22.6 million parameters, which is comparable to the baseline binary ViT (DeiT-S) but significantly less than other competitive models such as ReActNet (21.8 million parameters for ResNet-34 backbone and 29.3 million parameters for MobileNet backbone).
		
		\item 
		\textbf{FLOPs (Floating-Point Operations):} BinaryViT performed $0.19 \times 10^8$ FLOPs, much lower than ReActNet and other competing models, further highlighting its efficiency.
	\end{enumerate}
	
	
	
	\subsection{Comparisons with State-of-the-Art (SOTA) Binary Models}
	The BinaryViT method was directly compared with other leading binary models, and the results are as follows:
	
	\begin{enumerate}
		\item 
		ReActNet (ResNet-34 backbone): Achieved 67.5\% top-1 accuracy on ImageNet-1k with $1.93 \times 10^8$ operations and 21.8 million parameters.
		
		\item 
		BinaryViT: Matched ReActNet’s accuracy of 67.7\%, but with significantly fewer operations $(0.79 \times 10^8$ vs. $1.93 \times 10^8)$ and similar parameter count (22.6 million).
		
		\item 
		ReActNet (MobileNet backbone): Achieved 70.1\% top-1 accuracy, while BinaryViT* (with full-precision patch embedding layers) closely followed with 70.6\% top-1 accuracy and fewer operations, making it highly competitive.
	\end{enumerate}
	
	
	
	\subsection{Impact of Architectural Enhancements}
	The authors tested the impact of each architectural enhancement introduced in BinaryViT. The individual contributions to the model’s performance are detailed as follows:
	
	\begin{enumerate}
		\item 
		\textbf{Global Average Pooling:} Replacing token pooling with global average pooling increased the top-1 accuracy from \textbf{48.5\% to 56.4\%}, demonstrating the value of incorporating information from all input tokens.
		
		\item 
		\textbf{Multiple Pooling Branches:} Adding multi-branch average pooling layers further improved the accuracy to \textbf{60.2\%}, showing that this design helps to enrich the representational power of the model.
		
		\item 
		\textbf{Affine Transformations:} Introducing affine transformations before residual connections (to balance feature scales) increased accuracy to \textbf{61.8\%}.
		
		\item 
		\textbf{Pyramid Structure:} Implementing the pyramid structure, which mimics CNNs by processing higher-resolution features early on, provided the biggest improvement, bringing accuracy up to \textbf{67.7\%}.
	\end{enumerate}
	
	
	
	\subsection{Reduction in Computational Complexity}
	One of the key improvements announced by the authors is the ability of BinaryViT to reduce computational complexity without sacrificing performance:
	
	\begin{enumerate}
		\item 
		\textbf{Lower Bit-Operations (BOPs)}: BinaryViT achieved a balance between bit-operations and floating-point operations, outperforming other methods in terms of efficiency.
		
		\item 
		\textbf{Efficient Scaling:} The pyramid structure, multi-branch pooling, and affine transformations ensure that the model remains computationally efficient while handling large-scale image datasets like ImageNet-1k.
	\end{enumerate}
	
	
	\subsubsection{Comparison Between Full-Precision and Binary Versions}
	The authors demonstrated that the proposed BinaryViT model maintains performance close to its full-precision counterpart:
	
	\begin{enumerate}
		\item 
		The full-precision DeiT-S achieves \textbf{79.9\%} top-1 accuracy.
		
		\item 
		The binary version of BinaryViT achieved \textbf{70.6\%} accuracy, closing much of the gap between full-precision and binary ViT models.
	\end{enumerate}
	
	
	\section{Overall Improvements}
	\begin{enumerate}
		\item 
		\textbf{Significant performance boost:} BinaryViT improves the accuracy of binary ViTs by 19.2\% compared to the baseline, making it competitive with binary CNNs.
		
		\item 
		\textbf{Reduced operations and parameters:} BinaryViT achieves competitive performance with a lower computational cost, making it ideal for edge devices.
		
		\item 
		\textbf{Innovative architecture:} The introduction of CNN-inspired elements such as global average pooling, multiple branches, affine transformations, and pyramid structures enhances the performance of binary ViTs without introducing convolutions.
	\end{enumerate}
	
	
	
	
	
	
	
	\chapter{Vision Transformer for Small-Size Datasets \cite{DBLP:journals/corr/abs-2112-13492}}
	This paper focuses on improving the performance of Vision Transformers (ViTs) on small datasets. ViTs, which have shown remarkable success in large-scale datasets, often struggle with small datasets due to their weak locality inductive bias. This bias is critical in image classification tasks as it allows models to focus on local relationships between pixels, which CNNs do well but ViTs lack.
	
	The authors propose two main techniques to address this issue:
	
	\section{Shifted Patch Tokenization (SPT)}
	This method aims to improve the tokenization process by spatially shifting image patches in different directions before feeding them into the model. This shift increases the receptive field of each token, allowing the ViT to capture more spatial relationships between neighboring pixels, which enhances the model's ability to understand local features in an image.
	
	\subsection{Previous Approach:}
	Traditional Vision Transformers divide an input image into non-overlapping patches and treat each patch as a token, which is then fed into the transformer for processing. This method lacks spatial awareness between adjacent patches because the patches are non-overlapping. In CNNs, the use of convolutional filters ensures that neighboring pixels are processed together, allowing the network to capture local spatial information. However, ViTs, without such mechanisms, have limited capacity to capture local context.
	
	\subsection{Proposed Change:}
	The authors introduce Shifted Patch Tokenization (SPT), which enhances the spatial relationship between image patches. The core idea behind SPT is to spatially shift an image in multiple directions (up-left, up-right, down-left, down-right) before dividing it into patches. These shifted versions of the image are then concatenated with the original image and passed through the tokenization process. This results in a larger receptive field for each patch, enabling the model to capture more spatial relationships between neighboring pixels.
	
	\begin{enumerate}
		\item 
		\textbf{Impact:} SPT improves the model’s ability to understand local pixel interactions, which is particularly important for smaller datasets where capturing fine details is crucial. By increasing the locality inductive bias, the ViT performs more like a CNN in terms of capturing local information, while still leveraging the benefits of self-attention.
	\end{enumerate}
	
	
	
	
	
	\section{Locality Self-Attention (LSA)}
	This technique adjusts the attention mechanism in ViTs to focus more on local regions of an image. LSA uses two strategies: diagonal masking (removing the attention between a token and itself) and learnable temperature scaling (sharpening the attention score distribution). These adjustments prevent the attention from becoming too smooth, forcing it to focus more locally, thus boosting the model’s ability to differentiate between important regions in an image.
	
	\subsection{Previous Approach:}
	In standard ViTs, the self-attention mechanism evaluates the relationship between all tokens in an image. While this approach is effective for large datasets, it tends to be inefficient for small datasets because it results in a uniform distribution of attention across tokens. This means that ViTs often fail to focus on the most relevant tokens, especially in smaller images where local details matter more. Additionally, the attention scores tend to be smoothed due to the use of high temperatures in the softmax function, making it harder for the model to attend to important local regions.
	
	\subsection{Proposed Change:}
	The authors introduce Locality Self-Attention (LSA), which modifies the attention mechanism in two significant ways:
	
	\begin{enumerate}
		\item 
		\textbf{Diagonal Masking:} This method excludes self-tokens from the attention process. In standard attention mechanisms, tokens often pay too much attention to themselves (self-tokens). Diagonal masking forces the model to focus on relationships between different tokens rather than giving undue weight to each token itself.
		
		\item 
		\textbf{Learnable Temperature Scaling:} The authors propose adding a learnable temperature parameter to the softmax function, allowing the model to sharpen the attention distribution. A lower temperature sharpens the attention scores, helping the model focus on the most important tokens, particularly in the local regions of an image.
		
		\item 
		\textbf{Impact:} These two changes together reduce the tendency of ViTs to spread attention too broadly across the entire image. Instead, the attention becomes more focused on local regions, improving the ability of the model to recognize patterns and details within smaller datasets. LSA makes the attention mechanism more fine-tuned, thus improving performance on small-scale data.
	\end{enumerate}




	\section{Comparison to Other Data-Efficient ViTs}
	The paper compares the proposed SPT and LSA techniques to prior data-efficient ViT models, such as:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT (Data-efficient Image Transformer):} DeiT introduced techniques like knowledge distillation and data augmentations to make ViTs more efficient for training on mid-sized datasets like ImageNet. While effective, it still relies on large datasets and does not specifically address issues with small datasets.
		
		\item 
		\textbf{T2T-ViT (Tokens-to-Tokens ViT):} T2T-ViT introduced overlapping patches to improve the spatial relationship between patches. However, it did not fully solve the locality inductive bias issue as it only slightly increased the receptive field of the tokens.
		
		\item 
		\textbf{PiT (Pooling-based Vision Transformer):} PiT introduced a hierarchical pooling structure similar to CNNs to generate multi-scale features, allowing for better generalization on smaller datasets. However, it still does not effectively capture fine-grained local spatial information like SPT and LSA.
	\end{enumerate}
	
	In contrast, the SPT and LSA techniques specifically address the locality inductive bias in a more targeted way by increasing the receptive field during tokenization (SPT) and making attention more locally focused (LSA). These changes allow the proposed ViT to learn from small datasets effectively without relying on external large-scale pre-training, which was a limitation of previous models.
	
	
	
	\section{Efficiency vs. Performance Trade-offs}
	\subsection{Previous Models:}
	Many of the prior ViT-based models aimed to improve performance but often at the cost of computational efficiency. For example, DeiT used knowledge distillation, and T2T employed a complex overlapping tokenization method, both of which added computational overhead.
	
	
	\subsection{Proposed Model:}
	The proposed BinaryViT maintains competitive performance without a significant increase in computational cost. The SPT technique increases the receptive field without introducing convolutions or pooling layers, and LSA fine-tunes the attention mechanism with minimal additional parameters. As a result, the authors claim that BinaryViT improves accuracy on small datasets while maintaining acceptable overhead in terms of computational complexity.
	
	
	\section{Performance Gains}
	The experimental results in the paper show that the proposed BinaryViT model achieves substantial performance improvements over both the standard ViT and prior data-efficient ViTs when tested on small datasets like CIFAR-100, Tiny-ImageNet, and ImageNet. The model achieves these gains primarily due to its improved ability to capture local spatial information, a limitation that previous models struggled with.
	
	For example:
	\begin{enumerate}
		\item 
		In CIFAR-100, the use of SPT and LSA leads to an accuracy improvement of around 3-4\% compared to the baseline ViT model.
		
		\item 
		In Tiny-ImageNet, BinaryViT improves accuracy by up to 4.08\%, making it highly competitive with state-of-the-art CNNs on small datasets.
		
		\item 
		Even on a mid-sized dataset like ImageNet, the proposed changes result in a performance boost of 1.06\% to 1.60\%, demonstrating that the improvements are not limited to only small datasets.
	\end{enumerate}
	
	
	\section{Overall Impact of the Proposed Changes}
	The changes proposed by the authors—Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA)—represent significant architectural improvements that specifically address the limitations of Vision Transformers on small datasets. By increasing the locality inductive bias, these techniques make ViTs more efficient and effective at capturing the fine details that are crucial for tasks involving smaller datasets, bridging the gap between CNNs and transformers in this space.
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Performance Improvements on Small Datasets}
	The authors evaluated their methods on various small datasets, such as CIFAR-10, CIFAR-100, Tiny-ImageNet, and SVHN. They compared the performance of standard ViT models with and without the proposed SPT and LSA modules. The key findings are:
	
	\begin{enumerate}
		\item 
		\textbf{CIFAR-100:} The accuracy improved by up to 3.43\% for the CaiT model and 4.01\% for the PiT model when using SPT and LSA.
		
		\item 
		\textbf{Tiny-ImageNet:} ViTs saw significant performance boosts, with up to 4.08\% improvement for the Swin Transformer and 4.00\% improvement for the baseline ViT.
		
		\item 
		\textbf{SVHN and CIFAR-10:} Moderate improvements were observed, with a maximum gain of around 1-2\% for some models.
	\end{enumerate}
	These results highlight that the proposed methods effectively improve ViT performance on small datasets, where the original ViT architectures struggle.
	
	\subsection{Improvements in ImageNet Performance}
	While the methods were primarily designed for small datasets, they were also tested on the larger ImageNet dataset to verify if the improvements generalize to mid-sized data. The results show that the proposed methods also enhance ViT performance on ImageNet:
	
	\begin{enumerate}
		\item 
		\textbf{ViT:} Performance increased by 1.60\%, achieving a top-1 accuracy of 71.55\% (compared to 69.95\% for the baseline ViT).
		
		\item 
		\textbf{PiT: }Improved by 1.44\%, reaching 77.02\% accuracy.
		
		\item 
		\textbf{Swin Transformer:} Gained 1.06\% in accuracy, reaching 81.01\%.
	\end{enumerate}
	These results indicate that SPT and LSA can enhance ViTs even on larger datasets like ImageNet, although their primary benefit is seen in smaller datasets.
	
	
	\subsection{Efficiency and Computational Overhead}
	One of the key advantages of the proposed methods is their minimal computational overhead. Despite the performance improvements, the added complexity from SPT and LSA is modest:
	
	\begin{enumerate}
		\item 
		\textbf{Throughput:} The proposed methods cause only slight reductions in throughput. For example, the addition of SPT and LSA caused a 1.12\% latency overhead for the ViT model, and similar small increases for other models.
		
		\item 
		\textbf{FLOPs and Parameters:} The increase in FLOPs (Floating Point Operations) and parameters was minimal, ensuring that the models remain efficient and deployable, even with the added improvements in locality inductive bias.
	\end{enumerate}
	
	
	\subsection{Ablation Study Results}
	The authors conducted an ablation study to demonstrate the individual contributions of SPT and LSA:
	
	\begin{enumerate}
		\item 
		SPT (Shifted Patch Tokenization): Improved performance independently by +1.43\% in Tiny-ImageNet.
		
		\item 
		LSA (Locality Self-Attention): Provided an independent boost of +3.60\% in Tiny-ImageNet.
		
		\item 
		Combining SPT and LSA: When both methods were applied together, the performance improvement reached +4.00\% in Tiny-ImageNet, showing a strong synergy between the two methods.
	\end{enumerate}
	This shows that each technique effectively increases the model's ability to capture local details, and when used together, they yield even greater performance gains.
	
	
	
	\subsection{Qualitative Improvements}
	In addition to quantitative results, the authors provided qualitative visualizations of the ViT models’ attention maps. They compared the attention scores of final class tokens with and without the proposed methods:
	
	\begin{enumerate}
		\item 
		\textbf{Object Shapes:} When SPT and LSA were applied, the attention maps better captured the object shapes, focusing more on the relevant parts of the image, and avoiding excessive attention on background elements.
		
		\item 
		\textbf{Sharper Attention:} The learnable temperature scaling in LSA sharpened the attention distribution, leading to more focused and accurate attention on the target objects in images.
	\end{enumerate}
	These qualitative results visually demonstrate that the proposed changes help the model better understand the structure of the images, especially on smaller datasets where fine-grained details are essential.
	
	
	
	\subsection{Comparison with State-of-the-Art (SOTA) Models}
	The authors compared their proposed ViT models (with SPT and LSA) against several state-of-the-art (SOTA) models, including CNN-based models like ResNet and EfficientNet. The results showed that:
	
	\begin{enumerate}
		\item 
		\textbf{SL-CaiT:} Achieved better performance than ResNet and EfficientNet on most small datasets (except CIFAR-10).
		
		\item 
		\textbf{SL-Swin:} Provided comparable or better performance than CNNs while maintaining higher throughput.
	\end{enumerate}
	These comparisons highlight the ability of the modified ViTs to close the performance gap with CNNs on small datasets, a space where CNNs have traditionally outperformed transformers.
	
	
	
	
	\section{Key Takeaways:}
	\begin{enumerate}
		\item 
		\textbf{Substantial accuracy improvements:} The proposed SPT and LSA methods significantly enhance the performance of ViTs on small datasets, with gains of up to 4.08\% on Tiny-ImageNet and 3-4\% on CIFAR-100.
		
		\item 
		\textbf{Minimal computational overhead:} Despite the improvements, the increase in latency and computational cost is minimal, making these methods practical for deployment.
		
		\item 
		\textbf{Generalization to larger datasets:} While primarily aimed at small datasets, SPT and LSA also improve ViT performance on mid-sized datasets like ImageNet, with gains of up to 1.60\%.
		
		\item 
		\textbf{ViT competitiveness with CNNs:} The proposed methods make ViTs competitive with CNNs in small dataset tasks, both in terms of accuracy and computational efficiency.
	\end{enumerate}
	
	In conclusion, the results and improvements from the proposed methods mark a significant advancement for ViTs in handling small datasets, overcoming their limitations in local feature extraction, and making them competitive with traditional CNN architectures.

	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{How to train your ViT? Data, Augmentation,
		and Regularization in Vision Transformers \cite{DBLP:journals/corr/abs-2106-10270}}
		
		
	This paper, explores the best ways to train Vision Transformers (ViTs) effectively by balancing the use of data augmentation, regularization, model size, and available computational resources. ViTs are powerful models for computer vision tasks like image classification, but they tend to rely heavily on large datasets and regularization techniques to avoid overfitting. This article aims to provide practical insights for improving ViT performance, especially for practitioners with limited data and computational budgets.
	
	
	\section{Data Augmentation and Regularization ("AugReg")}
	The study shows that using proper data augmentation and regularization can yield models that perform as well as those trained on much larger datasets. By fine-tuning these techniques, smaller datasets can be used effectively, making the training process more efficient.
	
	\subsection{Previous Works:}
	Earlier studies on Vision Transformers, such as the original ViT paper, focused heavily on the need for large datasets like ImageNet-21k or JFT-300M to achieve competitive performance. The use of data augmentation and regularization was acknowledged, but the specific impact of these techniques on different dataset sizes, model configurations, and compute budgets was not systematically explored.
	
	\subsection{Proposed Changes}
	The authors of this paper shift the focus to a systematic study of how data \textbf{augmentation and regularization} can act as powerful tools to improve the performance of ViTs, even when the available dataset is smaller. The idea is that well-applied augmentation techniques (like Mixup and RandAugment) and regularization methods (such as dropout and stochastic depth) can compensate for the lack of large datasets, mimicking the effects of increasing the dataset size.
	
	
	This approach differs from previous work by providing empirical evidence showing that with carefully chosen augmentation and regularization settings, models can achieve results comparable to those trained on much larger datasets. This is particularly relevant for practitioners with limited access to massive datasets.
	
	
	
	
	
	
	\section{Trade-offs Between Data, Augmentation, and Compute Budget}
	 The article systematically investigates how the size of training data, the use of augmentation and regularization, and the compute budget interact. It demonstrates that well-designed regularization and augmentation strategies can mimic the effect of significantly increasing the dataset size.
	 
	 \subsection{Previous Works:}
	 Many earlier studies on ViTs, such as the DeiT (Data-efficient Image Transformers) work, emphasized the importance of using teacher-student distillation to enhance the performance of ViTs on smaller datasets. While this approach improved results, it added complexity to the training pipeline. Additionally, previous work often considered fixed trade-offs between model size and dataset size, without systematically exploring the effect of compute budget and regularization across a wide range of scenarios.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors go beyond distillation-based techniques and offer a more comprehensive investigation into the \textbf{interplay between model size, data size, and compute budget}. They conduct experiments across different ViT variants (from small to large models) and different dataset sizes (ImageNet-1k and ImageNet-21k) while systematically adjusting the amount of compute and AugReg techniques.
	 
	 
	 This approach offers a more nuanced understanding of how to balance \textbf{model complexity, data augmentation, and regularization} to achieve optimal performance under various constraints, helping practitioners make better decisions based on their available resources.
	 
	 
	 
	 
	 
	 
	 \section{Regularization Techniques and Their Impact}
	 \subsection{Previous Works:}
	 The role of regularization in ViTs was relatively underexplored in previous work, with most efforts focusing on training larger models on massive datasets. Dropout and stochastic depth were sometimes applied, but their effects were not systematically tested across different model sizes and dataset conditions.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors explore the use of \textbf{regularization techniques} like dropout and stochastic depth in greater detail. They find that regularization primarily benefits larger models when trained for extended periods, and actually harms performance in smaller models or when training on smaller datasets like ImageNet-21k. They also conduct ablation studies to identify the best settings for regularization, determining that a peak dropout/stochastic depth probability of 0.1 works best.
	 
	 
	 
	 This more detailed exploration of regularization sets their work apart by offering actionable insights into when and how to apply regularization effectively in ViTs, providing a deeper understanding of its benefits and drawbacks.
	 
	 
	 
	 
	 
	 
	 
	 
	 \section{Impact of Model Size}
	 Larger models tend to benefit more from regularization techniques, but this comes with the cost of requiring more training time and computational resources. Smaller models, on the other hand, might not benefit as much from regularization and could even suffer a loss in performance.
	 
	 \subsection{Previous Works:}
	 Earlier studies on ViTs often treated model size as a static factor, with larger models generally preferred when training on large datasets. However, there was little guidance on how to adapt model size based on the available compute or dataset size.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors provide specific \textbf{model size recommendations} based on their findings. They suggest that \textbf{larger patch sizes (e.g., 32×32)} are often more effective than reducing model size (e.g., using "Tiny" ViT models) when compute resources are limited. This means that instead of making the model smaller, increasing the patch size can help maintain performance without increasing the computational load significantly.
	 
	 This recommendation is based on a systematic analysis of throughput, model size, and patch size, providing a practical guide for selecting the right model configuration depending on the computational constraints.
	
	
	
	
	
	
	
	
	
	
	\section{Pre-training and Transfer Learning}
	The study finds that models pre-trained on larger datasets, like ImageNet-21k, perform better across a variety of tasks, including transfer learning. However, practitioners are advised to carefully choose augmentation and regularization settings to match their available compute budget and dataset size.
	
	\subsection{Previous Works:}
	Earlier ViT papers focused heavily on large-scale pre-training (e.g., JFT-300M) and emphasized the importance of pre-training on massive datasets for transfer learning. However, this left smaller organizations or researchers without access to such data at a disadvantage.
	
	
	\subsection{Proposed Changes:}
	The authors show that \textbf{transfer learning from pre-trained models on smaller datasets (like ImageNet-1k or ImageNet-21k)}, combined with strong AugReg, can yield results similar to those obtained from pre-training on massive datasets. They provide practical recommendations for selecting pre-trained models and fine-tuning them on specific tasks, which is particularly helpful for practitioners without access to large pre-training datasets.
	
	
	This change democratizes the use of ViTs, making them more accessible to a broader range of users and use cases, and helping users achieve competitive performance without requiring access to extremely large datasets for pre-training.
	
	
	\section{Practical Recommendations}
	The authors offer several practical guidelines, such as preferring data augmentation over regularization for smaller datasets, and choosing larger models with proper augmentation for the best results in a transfer learning setup.
	
	
	
	
	\section{Overall Impact of Changes}
	These changes collectively offer a more flexible and practical approach to training Vision Transformers, making them more applicable to real-world scenarios with constrained resources. The authors provide a comprehensive guide on how to balance data augmentation, regularization, and compute budget, allowing practitioners to achieve top-tier performance without relying on enormous datasets or computational resources. This is a significant shift from earlier ViT models, which focused primarily on large-scale data and heavy compute environments.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Training data-efficient image transformers \& distillation through attention \cite{DBLP:journals/corr/abs-2012-12877}}
	
	This paper focuses on making Vision Transformers (ViTs) more accessible and efficient by reducing their reliance on large datasets and expensive computing resources. Vision Transformers are highly effective for image classification, but their performance typically depends on massive datasets and extended training times on large infrastructure. This paper aims to train transformers effectively using only the ImageNet dataset on standard hardware, making these models more usable for a broader audience.
	
	
	
	\section{Data-Efficient Image Transformers (DeiT)}
	The authors propose a new training method for Vision Transformers, called DeiT, which allows these models to achieve high performance using only the ImageNet-1k dataset, without the need for the massive datasets like JFT-300M used in earlier ViT models. They demonstrate that it is possible to train ViTs efficiently on a single computer with 4 to 8 GPUs within just a few days, making them competitive with convolutional neural networks (CNNs) on standard benchmarks.
	
	\subsection{Previous Works:}
	The original ViT model by Dosovitskiy et al. (2020) showed exceptional performance, but it required training on extremely large datasets like JFT-300M, which contains 300 million labeled images. This makes the model less accessible for researchers and organizations with limited computational resources. The training of ViT models also demanded significant infrastructure, often involving many GPUs over long periods.
	
	\subsection{Proposed Changes:}
	The authors propose the DeiT (Data-efficient Image Transformer), a novel approach for training ViTs more efficiently. Key changes in the training strategy include:
	\begin{enumerate}
		\item 
		\textbf{Training on ImageNet-1k only:} The authors demonstrate that it is possible to train ViTs using only the 1.28 million images in the ImageNet-1k dataset, instead of relying on large-scale private datasets.
		
		\item 
		\textbf{Shorter Training Time:} The authors successfully trained their models on a single machine with 4 to 8 GPUs in less than three days, significantly reducing the computational cost and making it accessible to a wider range of users.
		
		\item 
		\textbf{Use of Repeated Augmentation:} Repeated augmentation was introduced to provide more data variations during training, allowing the model to generalize better on smaller datasets. This is crucial for data efficiency in training without relying on external datasets.
	\end{enumerate}
	By optimizing the training setup, the DeiT model becomes more practical for real-world use cases, where large-scale datasets and high-end computing resources are unavailable.
	
	
	
	
	
	
	
	\section{Distillation Through Attention}
	A unique contribution of this paper is the introduction of a distillation token, a new method that enables the student model (transformer) to learn from a teacher model (either a CNN or another transformer). This token is added to the transformer’s input and interacts with the class token during training. The distillation token improves the model's performance by helping it mimic the predictions of the teacher, allowing the transformer to learn more efficiently from the teacher’s inductive biases, especially when the teacher is a CNN.
	
	\subsection{Previous Works:}
	Knowledge distillation is a common technique used in CNN models, where a smaller "student" model learns from a larger "teacher" model, typically by mimicking the teacher's output (soft labels). Previous works used this approach for model compression and transfer learning, but it was not specifically adapted to the transformer architecture.
	
	
	\subsection{Proposed Changes:}
	The authors introduce a novel distillation mechanism specifically designed for transformers. The key innovation is the distillation token, which operates alongside the class token. Unlike the typical class token that learns from the ground truth labels, the distillation token learns from the teacher model’s predictions, allowing the student transformer to benefit from the teacher's inductive biases.
	
	\begin{enumerate}
		\item 
		\textbf{Interaction Through Attention:} The distillation token interacts with the other tokens through the self-attention mechanism, ensuring that the student model receives rich, token-level information from the teacher.
		
		\item 
		\textbf{Inductive Bias Transfer:} Interestingly, the paper shows that a CNN teacher (such as RegNet) transfers its inductive biases (such as convolutional feature learning) to the transformer through this attention-based distillation process, making the transformer perform better on image recognition tasks.
	\end{enumerate}
	This approach to distillation is a significant departure from earlier methods because it leverages the attention mechanism to integrate the teacher's guidance into the student's learning process, improving the model's performance.
	
	
	
	\section{Smaller and More Efficient Models (DeiT-S and DeiT-Ti)}
	\subsection{Previous Works:}
	The original ViT model introduced a large transformer architecture (ViT-B) that required massive datasets and significant computational resources to achieve competitive results. Smaller versions of ViTs had not been thoroughly explored, and the effectiveness of scaling down transformers while maintaining performance was not well established.
	
	
	\subsection{Proposed Changes:}
	The authors introduce two smaller versions of the ViT model in their DeiT approach:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-S (Small):} A smaller version with fewer parameters and heads, designed to be more efficient while still maintaining competitive performance.
		
		\item 
		\textbf{DeiT-Ti (Tiny):} An even smaller model, comparable to ResNet-18 in terms of parameter count, which is designed to be lightweight and fast while achieving solid accuracy on image classification tasks.
	\end{enumerate}
	These smaller models are trained using the same data-efficient strategies and distillation techniques as the larger DeiT-B model, making them highly competitive with traditional CNNs like ResNet. The development of these smaller models is a step forward in making transformers accessible for deployment on edge devices and environments with limited computational power.
	
	
	
	
	\section{Performance and Efficiency Gains}
	The proposed DeiT models achieve competitive results with state-of-the-art CNNs, reaching up to 85.2\% top-1 accuracy on ImageNet while requiring less compute time and fewer resources. The authors show that, with their distillation method, Vision Transformers can even outperform their teacher models in some cases, demonstrating the effectiveness of this distillation approach.
	
	
	\subsection{Previous Works:}
	Earlier Vision Transformer models (such as ViT-B and ViT-L) were known for their large computational overhead. Their training and inference speed were slower than state-of-the-art CNN models like EfficientNet and ResNet, limiting their usability in real-time or resource-constrained environments.
	
	
	\subsection{Proposed Changes:}
	The authors focus on improving the \textbf{throughput} (images processed per second) of DeiT models, ensuring that they are efficient in terms of both \textbf{accuracy} and \textbf{speed}. They show that:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-S} achieves higher throughput than many large CNNs like EfficientNet-B4 while maintaining competitive accuracy.
		
		\item 
		\textbf{The DeiT-B} model surpasses earlier ViTs trained on ImageNet-1k in terms of throughput, closing the gap between transformers and CNNs.
		
		\item 
		Through their distillation process, the authors achieve \textbf{better accuracy-to-throughput ratios} than standard CNNs, meaning that transformers can now offer high performance without compromising speed.
	\end{enumerate}
	
	
	
	\section{Transfer Learning and Generalization}
	The authors also explore the transfer learning capabilities of the DeiT models, showing that they perform well when fine-tuned on other popular datasets like CIFAR-10, CIFAR-100, and iNaturalist, further proving their generalization power.
	
	\subsection{Previous Works:}
	Vision Transformers, especially in their original form, struggled with transfer learning performance when fine-tuned on smaller datasets, often requiring extensive fine-tuning or retraining with external datasets. This was a significant limitation compared to CNNs, which have long been effective at transferring learned features across tasks and domains.
	
	
	\subsection{Proposed Changes:}
	The DeiT models are shown to \textbf{generalize well} to downstream tasks through transfer learning, performing competitively on popular datasets such as CIFAR-10, CIFAR-100, and iNaturalist. Key improvements include:
	
	\begin{enumerate}
		\item 
		\textbf{Fine-tuning at higher resolutions:} The authors successfully fine-tune DeiT models at different image resolutions, demonstrating that they can achieve high accuracy even when transferred to tasks that require different input sizes.
		
		\item 
		\textbf{Cross-dataset generalization:} DeiT models achieve competitive performance across various fine-grained classification tasks, proving their versatility beyond ImageNet. The model can be fine-tuned efficiently on smaller datasets, making it a more general-purpose model.
	\end{enumerate}
	This shows that DeiT not only excels at training from scratch but also performs strongly in transfer learning scenarios, making it more suitable for a broader range of tasks compared to previous ViT models.
	
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Competitive Performance with Smaller Datasets}
	The primary contribution of this work is that it shows Vision Transformers (ViTs) can achieve competitive results using only the ImageNet-1k dataset. Previous ViTs required massive datasets like JFT-300M for training. The authors achieve the following:
	\begin{enumerate}
		\item 
		\textbf{DeiT-B} (Base model) achieves \textbf{83.1\% top-1 accuracy} on ImageNet-1k, with no external data.
		
		\item 
		When fine-tuned at higher resolution (384×384), \textbf{DeiT-B} achieves \textbf{85.2\% top-1 accuracy}, surpassing many state-of-the-art CNN models like ResNet and EfficientNet.
	\end{enumerate}
	
	These results highlight that DeiT models can perform on par with models that rely on much larger datasets, making ViTs more accessible to the broader machine learning community.
	
	
	\subsection{Distillation Through Attention Enhances Model Performance}
	The introduction of the \textbf{distillation token} is a novel improvement in this work. This token allows the transformer model to learn from a teacher (often a CNN) in a more effective manner than traditional distillation methods. The benefits of this approach include:
	\begin{enumerate}
		\item 
		\textbf{DeiT-B with distillation (DeiT-B)} achieves \textbf{84.4\% top-1 accuracy} on ImageNet-1k, which is higher than the original ViT-B model (trained on larger datasets) and better than CNN models of similar size.
		
		\item 
		The distillation token outperforms standard soft and hard distillation techniques, showing that it is particularly effective in helping the transformer model adopt the inductive biases from a CNN teacher.
	\end{enumerate}
	This result demonstrates that transformers can benefit from distilling knowledge from CNNs, making the training process more efficient and effective.
	
	
	
	\subsection{Improved Throughput and Computational Efficiency}
	Another key improvement is the \textbf{throughput and computational efficiency} of the DeiT models. Compared to earlier Vision Transformers, the DeiT models:
	
	\begin{enumerate}
		\item 
		Achieve faster \textbf{throughput while maintaining high accuracy}. For example, DeiT-Tiny (DeiT-Ti), the smallest model, processes 2536 images per second, making it one of the fastest ViTs available.
		
		\item 
		\textbf{Outperform ViTs trained on larger datasets} in terms of the balance between accuracy and throughput. The DeiT models are competitive with CNNs like EfficientNet and ResNet in terms of both speed and accuracy.
	\end{enumerate}
	The improved throughput and reduced computational requirements make DeiT models more suitable for real-world applications, especially in environments with limited computational resources.
	
	
	
	
	\subsection{Smaller Models with Comparable Accuracy}
	The paper introduces smaller and more efficient variants of the DeiT model:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-S (Small model)} achieves \textbf{79.8\% top-1 accuracy} on ImageNet-1k while having fewer parameters than ResNet-50, making it a strong alternative to CNNs for image classification tasks.
		
		\item 
		\textbf{DeiT-Ti (Tiny model)}, a smaller version comparable to ResNet-18, achieves \textbf{72.2\% top-1 accuracy} on ImageNet-1k, showing that even small ViT models can be effective when trained efficiently.
	\end{enumerate}
	
	These smaller models provide flexibility in choosing the right trade-off between model size, speed, and accuracy, depending on the task at hand.
	
	
	
	
	\subsection{Transfer Learning and Generalization}
	The authors demonstrate that DeiT models generalize well to downstream tasks, achieving competitive performance in transfer learning scenarios. Key results include:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-B} \textbf{achieves 99.1\% accuracy on CIFAR-10, 91.4\% on CIFAR-100}, and \textbf{93.9\% on Stanford Cars}, which are common benchmarks for transfer learning tasks.
		
		\item 
		The models also perform well on fine-grained classification tasks and datasets like iNaturalist, Oxford-102 Flowers, and Stanford Cars.
	\end{enumerate}These results show that the DeiT models are not only effective for large-scale classification tasks like ImageNet but also excel in transfer learning, making them versatile for different types of datasets and tasks.
	
	
	
	
	\subsection{Training Time Reduction}
	One of the critical improvements in this method is the reduced training time:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-B} is trained on \textbf{a single machine with 4 to 8 GPUs} in just three days, compared to the weeks or even months of training required for other ViT models on large datasets.
		
		\item 
		The fine-tuning of DeiT models at higher resolutions is also efficient, taking only around 20 hours on an 8-GPU machine for 25 epochs.
	\end{enumerate}
	This reduction in training time, combined with efficient throughput, makes DeiT models more practical and accessible for organizations with limited resources.
	
	
	
	\subsection{Distillation from CNNs Is More Effective than from Transformers}
	A surprising finding in the paper is that \textbf{transformers learn better when the teacher is a CNN rather than another transformer}. The authors observed that:
	
	\begin{enumerate}
		\item 
		When using a \textbf{RegNetY CNN} as the teacher, the student DeiT model outperforms models distilled from transformer teachers.
		
		\item 
		The distillation process allows the transformer to learn important inductive biases from the CNN, improving its ability to capture spatial and hierarchical information in images.
	\end{enumerate}
	
	This result suggests that CNNs can still play a valuable role in improving transformer models through distillation, leveraging the strengths of both architectures.
	
	
	
	\section{Overall Improvements}
	\begin{enumerate}
		\item 
		\textbf{Data Efficiency:} The DeiT models reduce the need for large datasets, showing that ViTs can perform well with ImageNet-1k only.
		
		\item 
		\textbf{Distillation Innovation:} The novel distillation token mechanism leads to significant performance gains, especially when CNNs are used as teachers.
		
		\item 
		\textbf{Improved Throughput:} DeiT models achieve a better balance between accuracy and throughput, making them faster and more efficient for real-time applications.
		
		\item 
		\textbf{Smaller, Faster Models:} The introduction of DeiT-S and DeiT-Ti offers more flexibility with smaller models that perform competitively.
		
		\item 
		\textbf{Transfer Learning Success:} DeiT models generalize well across various downstream tasks, proving to be versatile for different datasets.
	\end{enumerate}
	
	
	

	
	
	
	
	
	
	
	
	
	
	
	\chapter{Going deeper with Image Transformers \cite{DBLP:journals/corr/abs-2103-17239}}
	This paper explores how to enhance the performance of Vision Transformers (ViTs) by making them deeper and more efficient for image classification tasks. The authors focus on improving the architecture of ViTs to address their limitations in handling complex visual tasks, especially when compared to Convolutional Neural Networks (CNNs).
	
	
	\section{Key Ideas:}
	\subsection{Deeper Vision Transformers (ViTs):}
	The paper investigates the performance of ViTs when scaling them to deeper layers. While transformers have excelled in natural language processing, the same depth and complexity have not been fully explored in vision tasks. The authors experiment with significantly deeper ViTs to better understand how increasing depth affects the model’s ability to capture detailed visual information.
	
	\subsection{Class-Attention Mechanism:}
	One of the major contributions of this work is the introduction of \textbf{class-attention layers}. These layers are added to the architecture to improve how the model attends to important image features. The class-attention mechanism allows the model to focus more on class-relevant areas of the image, leading to better performance, especially when dealing with complex, high-resolution images.
	
	\subsection{Distillation with Class-Attention:}
	The authors explore \textbf{knowledge distillation}—a technique where a smaller or less powerful "student" model learns from a larger, pre-trained "teacher" model. While previous works used a distillation token in Vision Transformers to enhance performance, this paper primarily uses hard-label distillation, which averages the teacher’s prediction with the true label. The class-attention layers, however, did not benefit from the distillation token as much as expected, so the authors opted for \textbf{hard-label distillation}, which provided better results.
	
	
	
	\subsection{Efficient Training and Generalization:}
	The paper also focuses on how these deeper ViTs, equipped with class-attention, can be trained efficiently and still generalize well to new tasks. By fine-tuning their model architecture, they show that transformers can handle large-scale vision tasks effectively, even with deeper networks. They also achieve faster convergence through the use of hard-label distillation.
	
	
	\subsection{Performance on Benchmarks:}
	The authors demonstrate that their deeper ViTs with class-attention perform exceptionally well on various image classification benchmarks, showing improvements over standard ViTs and CNNs. Their models achieve higher accuracy while maintaining computational efficiency, making them more practical for real-world applications.
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Performance Improvement with Depth}
	A major finding of the paper is that scaling ViTs to deeper architectures improves performance on image classification tasks. By extending the depth of Vision Transformers, the model is able to capture more complex visual patterns and hierarchical features, leading to better results. The authors demonstrate that:
	
	\begin{enumerate}
		\item 
		Deeper ViTs with \textbf{up to 100+} layers outperform shallower models, especially when trained on large datasets.
		
		\item 
		\textbf{Deeper transformers} are shown to be highly competitive with state-of-the-art CNN architectures while retaining the flexibility and scalability that ViTs offer.
	\end{enumerate}
	This improvement highlights the ability of ViTs to handle complex visual tasks effectively when scaled to deeper architectures.
	
	
	\subsection{Introduction of Class-Attention Layers}
	The authors propose \textbf{class-attention layers}, which are a significant architectural improvement. These layers are designed to:
	\begin{enumerate}
		\item 
		Focus more on class-relevant areas of an image, helping the model attend to important features while ignoring irrelevant information.
		
		\item 
		Improve the model's ability to distinguish between fine-grained classes, making it more effective for high-resolution and complex images.
	\end{enumerate}
	The introduction of class-attention layers results in \textbf{improved accuracy} in image classification benchmarks compared to standard ViT models. By better aligning the model’s attention with class labels, the classification performance is enhanced, particularly for tasks that require precise attention to detail.
	
	
	\subsection{Hard-Label Distillation for Faster Convergence}
	In contrast to previous work that relied on distillation tokens, the paper shows that \textbf{hard-label distillation}—a method where the model's predictions are averaged with the teacher’s output—provides better performance, particularly when used with class-attention layers. The authors found that:
	\begin{enumerate}
		\item 
		\textbf{Hard-label distillation} led to \textbf{faster convergence} during training, reducing the overall training time required to achieve high accuracy.
		
		\item 
		The method is more effective for deeper transformers, as it simplifies the distillation process without sacrificing accuracy.
	\end{enumerate}
	This improvement demonstrates that hard-label distillation can help models learn more efficiently from pre-trained teachers, particularly when deeper architectures are used.
	
	
	\subsection{Training Efficiency and Generalization}
	The deeper ViTs proposed in this paper are not only more accurate but also \textbf{more efficient in training}. The authors optimized the training process to handle the deeper architectures, ensuring that the model can still converge quickly and generalize well to new tasks. Key results include:
	
	\begin{enumerate}
		\item 
		\textbf{Faster convergence} during training compared to traditional ViTs, thanks to the combined use of class-attention layers and hard-label distillation.
		
		\item 
		The deeper models generalize well to different datasets and achieve strong performance across various benchmarks without overfitting, demonstrating the robustness of the proposed architecture.
	\end{enumerate}
	
	
	
	\subsection{Benchmark Results and Competitive Performance}
	The authors tested their proposed deeper ViTs on several image classification benchmarks, showing \textbf{improved performance over baseline models:}
	
	\begin{enumerate}
		\item 
		The model achieves higher accuracy compared to standard Vision Transformers, ResNet-based CNNs, and previous state-of-the-art models.
		
		\item 
		When combined with class-attention and hard-label distillation, the deeper ViTs achieve \textbf{competitive results on benchmarks like ImageNet} and other challenging datasets.
	\end{enumerate}
	These results position deeper ViTs with class-attention as a strong alternative to CNNs, making them viable for large-scale and complex visual tasks.
	
	
	
	\subsection{Improved Attention Mechanism for Class Prediction}
	The introduction of the class-attention mechanism ensures that the \textbf{model pays more attention to class-relevant features}, reducing the chance of focusing on background noise or irrelevant parts of an image. This improves the accuracy of class predictions, especially in complex datasets where fine-grained differences between classes are crucial.
	
	\begin{enumerate}
		\item 
		The \textbf{class-attention mechanism} ensures better feature extraction and significantly enhances the model's performance in distinguishing between closely related classes.
	\end{enumerate}
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Attention is All you need \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}
	This paper introduces the Transformer model, a new architecture for sequence transduction tasks like machine translation, which relies entirely on self-attention mechanisms instead of the traditional recurrent or convolutional layers commonly used in previous models. This approach aims to address the limitations of recurrent neural networks (RNNs), particularly their inefficiency in parallelization and the challenges they face in learning long-range dependencies within sequences.
	
	
	\section{Transformer Architecture:}
	The Transformer is designed with both an encoder and decoder, which are stacked in layers. Each layer uses a self-attention mechanism that allows the model to focus on different parts of the input sequence, regardless of their distance. This is different from RNNs and Convolutional Neural Networks (CNNs), where longer sequences pose challenges for learning relationships between distant elements.
	
	\section{Self-Attention and Multi-Head Attention:}
	At the core of the Transformer is the \textbf{self-attention (SA)} mechanism, where each word in a sequence attends to every other word, learning the relationships between them. The model uses \textbf{multi-head attention (MHA)}, which runs several attention mechanisms in parallel, allowing the model to jointly focus on different parts of the sequence in multiple ways. This enables the model to capture richer contextual information and improve performance on tasks like language translation.
	
	\section{Positional Encoding:}
	Since the model is not based on sequences or recurrence, it lacks an inherent sense of order. The authors introduce \textbf{positional encodings} to provide the model with information about the position of each word in the input sequence. This helps the Transformer distinguish between different words' positions within a sentence and ensures that word order is maintained.
	
	
	\section{Advantages of Transformers:}
	The Transformer architecture is highly \textbf{parallelizable} and can be trained much faster than RNN-based models, which process one word at a time. By using self-attention, the model reduces the computational complexity and makes it easier to train on long sequences. This leads to faster training times, greater scalability, and the ability to achieve high performance with significantly fewer resources compared to other models.
	
	
	\section{Results:}
	The Transformer achieves state-of-the-art results on machine translation tasks, including \textbf{English-to-German} and \textbf{English-to-French} translations. It significantly outperforms previous models like RNNs and CNNs in terms of both accuracy (measured by BLEU score) and efficiency (measured by training speed). The model can be trained in just a few days on standard hardware while delivering superior performance.
	
	
	\section{Summary}
	In essence, the Transformer model redefines how sequence modeling is approached by eliminating recurrence and relying entirely on attention mechanisms. This new architecture is not only more efficient and parallelizable but also capable of learning long-range dependencies better than previous models. The paper’s results demonstrate the Transformer’s superiority in machine translation tasks, making it a breakthrough in the field of natural language processing.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Deepfake Video Detection Using Convolutional Vision Transformer \cite{DBLP:journals/corr/abs-2102-11126}}
	
	This paper addresses the growing concerns around deepfakes—hyper-realistic videos created through deep learning techniques that can manipulate or replace faces in videos. While deepfakes have useful applications in fields like entertainment, education, and virtual reality, they also pose serious risks, such as being used for identity theft, misinformation, or fraud.
	
	The authors propose a new model for detecting deepfakes, called the \textbf{Convolutional Vision Transformer (CViT)}. This model combines the strengths of two powerful architectures: Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). CNNs are effective at learning local features (such as textures or small patterns), while ViTs excel at capturing global features and understanding the relationship between different parts of an image. By combining both models, CViT is able to detect subtle visual differences between real and fake videos.
	
	\section{Key Components of the Model}
	\subsection{Feature Learning through CNNs:}
	Feature Learning through CNNs: The CNN module extracts local features from video frames, such as facial details and textures, which are crucial for detecting visual artifacts commonly found in deepfakes.
	
	
	\subsection{Global Feature Understanding through ViTs:}
	The Vision Transformer component processes the features extracted by the CNN, learning the relationships between different parts of the image. This helps the model detect manipulations at both a local and global level.
	
	
	\subsection{Comprehensive Data Preprocessing:}
	The authors emphasize the importance of data preprocessing, which ensures that the input data is well-prepared for training. This step helps improve the model's accuracy and robustness.
	
	
	\subsection{Testing and Results:}
	 The CViT model was trained and tested on the DeepFake Detection Challenge (DFDC) dataset, one of the largest and most diverse datasets for detecting deepfakes. The model achieved an accuracy of 91.5\%, demonstrating strong performance in identifying fake videos. However, the authors acknowledge that there is room for improvement and plan to expand their research by using more diverse datasets in the future.
	
	
	
	
	\section{Results and Improvements}
	This paper presents several key results and improvements from the proposed Convolutional Vision Transformer (CViT) model, which combines CNNs and Vision Transformers for detecting deepfake videos. Below is a detailed overview of the results and improvements:
	
	\subsection{High Accuracy in Deepfake Detection}
	The CViT model achieved 91.5\% accuracy on the DeepFake Detection Challenge (DFDC) dataset, demonstrating its effectiveness in distinguishing between real and fake videos. This high accuracy reflects the model's ability to detect subtle artifacts and inconsistencies in manipulated videos, which are often difficult for simpler models to detect.
	
	
	\subsection{AUC and Loss Metrics}
	Along with the high accuracy, the model also achieved an AUC (Area Under the Curve) value of 0.91 and a loss value of 0.32. These metrics further validate the robustness of the model in detecting deepfakes. The AUC score indicates that the model performs well in distinguishing true positives (real videos) from false positives (deepfakes), and the low loss value shows that the predictions are close to the actual values.
	
	
	\subsection{Combination of CNN and ViT for Local and Global Feature Learning}
	One of the main improvements of the CViT model is its ability to combine the strengths of CNNs and Vision Transformers (ViTs). CNNs are highly effective at extracting local features such as textures and small details, while ViTs are powerful in understanding global relationships across an image. The fusion of these two methods allows CViT to detect deepfakes more effectively by capturing both fine-grained details and broader spatial relationships in video frames.
	
	
	This combined approach is a notable improvement over models that rely solely on CNNs or Transformers, as it provides a more comprehensive analysis of the video content.
	
	
	\subsection{Generalized Model for Different Deepfake Scenarios}
	The authors describe their model as "generalized" for several reasons:
	\begin{enumerate}
		\item 
		\textbf{Local and Global Feature Learning:} The combination of CNN and ViT enables the model to learn from both small details and larger spatial contexts, making it more versatile in detecting various types of deepfakes.
		
		\item 
		\textbf{Thorough Data Preprocessing:} The emphasis on proper data preprocessing ensures that the input data is of high quality, leading to better detection performance.
		
		\item 
		\textbf{Diverse Dataset Training:} The CViT model was trained on the DFDC dataset, which includes a wide variety of videos created in different environments, lighting conditions, and orientations. This makes the model more adaptable to real-world deepfake scenarios.		
	\end{enumerate}
	
	
	\subsection{Comparison with Other Models}
	The paper compares the performance of the CViT model with other existing deepfake detection models and shows that:
	\begin{enumerate}
		\item 
		The CViT model performs better than simpler CNN-based models, such as those using RNNs or shallow CNNs.
		
		\item 
		The model demonstrates a higher accuracy on most subsets of the DFDC dataset, such as FaceForensics++, where it achieves high detection rates on multiple types of manipulations (Deepfake, FaceSwap, and others).
	\end{enumerate}
	However, the authors note that the model performed less effectively on certain datasets, such as FaceForensics++ FaceShifter, indicating room for further improvement.
	
	
	
	\subsection{Data Preprocessing and Face Extraction}
	The authors emphasize the importance of data preprocessing, especially in the extraction of facial images from video frames. By carefully pre-processing the data, including extracting faces in a standardized format (224x224 pixels), the model is able to focus on the relevant parts of the video, improving detection accuracy.
	
	
	The use of tools like BlazeFace and MTCNN for face detection and alignment ensures that the model works with high-quality input data. This careful preprocessing approach is a key factor in the model’s strong performance, setting it apart from other deepfake detection models that may not prioritize preprocessing to the same extent.
	
	
	
	
	\subsection{Future Improvements and Expansion}
	While the model shows strong results, the authors acknowledge that there is still room for improvement. They plan to enhance the model by:
	
	\begin{enumerate}
		\item 
		Adding more diverse datasets for training, which will help the model detect deepfakes in even more varied scenarios.
		
		\item 
		Improving artifact detection in more challenging deepfake videos, such as those created using advanced techniques like FaceShifter, where the model currently struggles.
	\end{enumerate}
	
	
	
	\section{Summary of Improvements:}
	\begin{enumerate}
		\item 
		High accuracy (91.5\%) and AUC score (0.91) on the DFDC dataset, showing effective deepfake detection.
		
		\item 
		CNN and ViT combination enables the model to capture both local and global features, improving the ability to detect deepfakes.
		
		\item 
		Generalized model design allows the CViT to perform well across different deepfake scenarios, including varying video qualities and environments.
		
		\item 
		Comprehensive data preprocessing ensures the input quality is high, leading to better model performance.
		
		\item 
		Future improvements aim to address current limitations, such as improving detection on more complex deepfake techniques and expanding the dataset.
	\end{enumerate}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Visual Transformer Pruning \cite{DBLP:journals/corr/abs-2104-08500}}
	This paper focuses on making Vision Transformers (ViTs) more efficient by reducing their size and computational demands without significantly sacrificing performance. Vision Transformers are known for their high accuracy in computer vision tasks like image classification, but they are resource-intensive, requiring substantial memory, computation, and storage. This makes them difficult to deploy on devices with limited resources, such as mobile phones and embedded systems.
	
	
	
	The authors propose a method to prune Vision Transformers, which means removing less important parts of the model to reduce the number of parameters and computational operations. This is done through L1 regularization, a technique that encourages sparsity in the model's dimensions. During training, this method automatically identifies which parts of the model (such as individual dimensions in the layers) are less important and can be removed. By applying this pruning technique, the authors manage to create smaller, faster, and more efficient Vision Transformers while maintaining competitive accuracy.
	
	
	\section{Key Components of the Approach}
	\begin{enumerate}
		\item 
		L1 Regularization for Sparsity: The method applies L1 regularization to the transformer’s dimensions, which forces the model to focus on the most important components while ignoring others. This results in a model with fewer, but more critical, dimensions.
		
		\item 
		Pruning Process: After training with L1 regularization, the dimensions with low importance are pruned. This means they are effectively removed from the model, reducing its size and computational complexity.
		
		\item 
		Fine-tuning After Pruning: Once the model is pruned, it is fine-tuned to ensure that the performance drop is minimized. This final step helps the model regain any accuracy lost during pruning.
		
		\item 
		Experiments and Results: The authors test their pruning method on two large datasets—ImageNet-100 and ImageNet-1K. The results show that the pruned models significantly reduce the number of parameters and computations, while maintaining similar accuracy to the original, unpruned models. For example, pruning 40\% of the model’s dimensions reduces computational costs by 43\% while only dropping accuracy by about 1%.
	\end{enumerate}
	
	
	\section{Results and Improvements}
	\subsection{Significant Reduction in Parameters and Computation Costs}
	The proposed pruning method successfully reduces both the number of parameters and the computational costs (measured in FLOPs) of Vision Transformers. For example:
	
	\begin{enumerate}
		\item 
		40\% pruning of the model’s dimensions resulted in a 45.3\% reduction in parameters and a 43.0\% reduction in FLOPs (computational cost).
		
		\item 
		20\% pruning reduced the parameters by 23.5\% and FLOPs by 22.0\%, showing the method’s ability to effectively compress the model.
	\end{enumerate}
	This demonstrates that the pruning approach drastically cuts the resource requirements of Vision Transformers, making them more suitable for deployment on devices with limited resources.
	
	
	\subsection{Maintaining High Accuracy with Minimal Loss}
	Despite reducing the model's size and complexity, the accuracy of the pruned models remained high, showing only minor drops in performance. Specifically:
	
	
	\begin{enumerate}
		\item 
		After pruning 40\% of the model’s dimensions, the accuracy on ImageNet-1K dropped by only 1.1\%, from 81.8\% to 80.7\%.
		
		\item 
		Pruning 20\% of the dimensions resulted in an accuracy drop of just 0.5\%, maintaining a high level of performance with significantly fewer parameters and lower computational cost.
	\end{enumerate}
	This result demonstrates the strength of the pruning method, as it allows for a substantial reduction in resources while keeping the model highly accurate.
	
	
	
	\subsection{Effectiveness on Large Datasets}
	The pruning method was tested on both ImageNet-100 (a subset of ImageNet-1K) and ImageNet-1K, two large-scale image classification datasets. The results were consistent across both datasets:
	
	
	\begin{enumerate}
		\item 
		On ImageNet-100, the pruned models achieved similar reductions in parameters and FLOPs with minimal accuracy loss. For instance, the 40\% pruned model maintained an accuracy of 92.58\% on ImageNet-100.
		
		\item 
		On the larger ImageNet-1K dataset, the pruned model remained highly competitive with only a 1.1\% accuracy loss despite the significant reduction in computational complexity.
	\end{enumerate}
	This demonstrates that the method generalizes well to large datasets, making it viable for use in real-world applications involving complex data.
	
	
	\subsection{Flexibility of Pruning Rates}
	The authors conducted ablation studies to test different pruning rates, allowing for flexible reductions in model size. Depending on the desired trade-off between resource savings and accuracy, users can select different pruning rates:
	
	
	\begin{enumerate}
		\item 
		20\% pruning saves a modest amount of computational resources while maintaining nearly the same level of accuracy.
		
		\item 
		40\% pruning significantly reduces resource usage while still keeping the accuracy drop minimal.
	\end{enumerate}
	This flexibility enables users to adjust the pruning rate based on the specific requirements of their application, such as maximizing efficiency for mobile devices or embedded systems.
	
	
	
	
	\subsection{Simplicity and Efficiency of the Pruning Process}
	The method is described as simple yet efficient. It uses L1 regularization during training to automatically identify less important dimensions in the Vision Transformer. Afterward, the model is pruned based on the learned importance scores, and a final fine-tuning step ensures that the performance remains high.
	
	The simplicity of this pruning pipeline—training with L1 regularization, pruning, and fine-tuning—makes it easy to implement and apply to various Vision Transformer models. It is less complex than other compression techniques, such as quantization or knowledge distillation, but achieves comparable improvements in resource efficiency.
	
	
	
	
	\subsection{Promising Future Improvements}
	The authors suggest that this pruning method could be extended in future research to further reduce other components of Vision Transformers, such as:
	
	\begin{enumerate}
		\item 
		Pruning attention heads within the multi-head self-attention layers.
		
		\item 
		Pruning transformer layers, which could lead to even more significant reductions in both parameters and computational costs.
		
		\item 
		These potential improvements open the door for even more efficient and compact Vision Transformers that can be deployed on a wider range of devices.
	\end{enumerate}
	
	
	\section{Overall Improvements:}
	\begin{enumerate}
		\item 
		Reduced parameters and FLOPs: Pruning 20-40\% of the model's dimensions results in a significant reduction in both parameters and FLOPs, making the model more efficient for real-world applications.
		
		\item 
		Minimal accuracy loss: The method achieves large savings in computational resources while maintaining high accuracy, with only minor performance drops (as low as 0.5\% in some cases).
		
		\item 
		Flexibility of pruning rates: Users can adjust the pruning rate depending on the trade-off they prefer between model size and performance, making the method adaptable to different use cases.
		
		\item 
		Simplicity and ease of implementation: The pruning method is straightforward to apply, relying on L1 regularization and a simple pruning and fine-tuning process.
		
		\item 
		Potential for future enhancements: The method can be expanded to further reduce other components like attention heads and transformer layers, providing even more opportunities for efficient Vision Transformers.
	\end{enumerate}
	
	
	
	
	
	
	
	
	
	
	\chapter{Scalable MatMul-free Language Modeling \cite{zhu2024scalablematmulfreelanguagemodeling}}
	This paper introduces a new approach to language modeling that focuses on reducing the computational cost and memory usage of deep learning models. Traditional models, like transformers, rely heavily on matrix multiplication (MatMul) operations, which are computationally expensive and require large amounts of memory. This makes them inefficient, especially for deployment on devices with limited resources, such as mobile phones or edge devices.
	
	
	The authors propose a MatMul-free language model that replaces traditional matrix multiplication operations with bitlinear layers, which are much more efficient. These bitlinear layers use ternary weights (values of -1, 0, and 1) instead of continuous weights, significantly reducing the computational demands of the model. By avoiding matrix multiplication, the model becomes faster and consumes less memory, making it scalable and suitable for real-time applications in resource-constrained environments.
	
	\section{Key Contributions}
	\subsection{Bitlinear Layers:}
	Instead of relying on standard matrix multiplication, the authors introduce bitlinear layers that perform more efficient operations using ternary weights. These layers simplify the model’s calculations while maintaining a similar level of performance to traditional models.
	
	\subsection{Ternary Weights:}
	The use of ternary weights (-1, 0, 1) further reduces computational complexity. Ternary weights reduce the precision of the model's parameters without significantly affecting accuracy, enabling faster inference and lower memory usage.
	
	\subsection{Scalability and Efficiency:}
	The model is scalable, meaning it can handle larger language modeling tasks effectively, despite its simpler structure. It is particularly well-suited for use on devices with limited resources, where traditional models would be too computationally expensive to run efficiently.
	
	\subsection{Experimental Results:}
	Through extensive experiments on benchmark datasets, the authors demonstrate that their MatMul-free model achieves performance comparable to traditional models, such as Transformer++, while being significantly more efficient in terms of memory and computation.
	
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Reduction in Computational Complexity}
	One of the main improvements of this method is the reduction in computational complexity:
	
	\begin{enumerate}
		\item 
		By eliminating matrix multiplication, which is traditionally the most computationally expensive operation in models like transformers, the proposed MatMul-free architecture significantly reduces both computation time and memory usage.
		
		\item 
		The introduction of bitlinear layers replaces traditional dense layers, allowing for more efficient processing of input data. This results in faster inference, which is crucial for real-time applications, especially on resource-constrained devices.
	\end{enumerate}
	
	
	\subsection{Memory Efficiency}
	Another key result is the reduction in memory usage:
	\begin{enumerate}
		\item 
		The model’s use of ternary weights (-1, 0, 1) in bitlinear layers reduces the precision of the model's parameters. However, this lower precision leads to a substantial reduction in memory requirements without significantly affecting model accuracy.
		
		\item 
		This memory efficiency makes the model particularly well-suited for deployment on edge devices, where memory resources are often limited.
	\end{enumerate}
	
	
	\subsection{Comparable Performance with Traditional Models}
	Despite the simpler architecture and fewer computations, the MatMul-free model achieves competitive performance when compared to traditional language models like Transformer++. The key findings include:
	
	\begin{enumerate}
		\item 
		On various language modeling benchmark tasks, the proposed model performs at a level that is comparable to traditional MatMul-based models.
		
		\item 
		Even with fewer parameters and reduced computation, the model maintains accuracy and is able to perform large-scale language modeling tasks effectively.
	\end{enumerate}
	This demonstrates that the proposed model can offer the same level of accuracy while using fewer computational resources, which is a significant advancement in terms of efficiency.
	
	
	\subsection{Faster Inference Times}
	The MatMul-free model provides a significant improvement in inference speed:
	
	\begin{enumerate}
		\item 
		The absence of matrix multiplication enables faster processing of inputs, which results in faster inference times compared to traditional transformer-based models. This makes the model suitable for real-time language modeling applications, where speed is critical.
		
		\item 
		The faster inference is particularly beneficial for deployment in low-power environments such as mobile devices, IoT devices, and edge computing, where computational power and speed are often constrained.
	\end{enumerate}
	
	
	\subsection{Scalability Across Model Sizes}
	The scalability of the MatMul-free model was evaluated through experiments on models with varying parameter sizes. The authors demonstrated that the architecture is highly scalable, meaning it can be used for both small-scale and large-scale language modeling tasks:
	
	\begin{enumerate}
		\item 
		The model performs well with different parameter sizes, indicating that it can be scaled up or down depending on the specific application requirements.
		
		\item 
		This scalability proves that the MatMul-free approach is flexible and adaptable to various use cases, from small language tasks to more complex, large-scale applications.
	\end{enumerate}
	
	
	\subsection{Potential for Further Optimization Through Quantization}
	While the proposed model is already efficient, the authors highlight the potential for further improvements by integrating more advanced quantization techniques:
	
	\begin{enumerate}
		\item 
		The use of ternary weights in the current model is a step towards quantization, but the authors suggest exploring more sophisticated quantization methods to further reduce computational and memory costs.
		
		\item 
		This opens the possibility of additional gains in efficiency, especially for tasks that require even more resource-constrained environments, such as embedded systems or real-time NLP applications.
	\end{enumerate}
	
	
	\subsection{Wider Applicability Beyond Language Modeling}
	The authors also hint at the possibility of extending their MatMul-free architecture to a broader range of natural language processing (NLP) tasks, beyond language modeling. Some potential areas for future exploration include:
	\begin{enumerate}
		\item 
		 Machine translation: The model’s efficiency could be leveraged in translation tasks where speed and low resource consumption are critical.
		
		\item 
		Question answering: The model’s scalability and efficiency make it a promising candidate for real-time question-answering systems, where both accuracy and quick response times are essential.
	\end{enumerate}
	
	
	\section{Overall Improvements:}
	\begin{enumerate}
		\item 
		Reduction in computational complexity: The elimination of matrix multiplication leads to faster and more efficient computation, making the model suitable for devices with limited computational resources.
		
		\item 
		Memory efficiency: By using ternary weights, the model reduces memory consumption without sacrificing much accuracy, enabling it to run on memory-constrained devices.
		
		\item 
		Comparable performance to traditional models: Despite its simpler architecture, the MatMul-free model achieves competitive results with traditional models like Transformer++, proving that it can handle large-scale language tasks effectively.
		
		\item 
		Faster inference times: The absence of matrix multiplication results in faster inference, which is critical for real-time applications, particularly in edge devices and mobile applications.
		
		\item 
		Scalability: The model scales well with different parameter sizes, demonstrating its adaptability for both small and large-scale tasks.
		
		\item 
		Opportunities for further optimization: The authors suggest further gains can be made by exploring additional quantization techniques, offering more potential for increased efficiency.
		
		\item 
		Wider applicability: The approach can be expanded to other NLP tasks beyond language modeling, such as machine translation and question answering, making it a versatile solution for various natural language processing applications.
	\end{enumerate}
	
	

	

	

	

	





	\chapter{SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks \cite{zhu2024spikegptgenerativepretrainedlanguage}}
	

	
	Spiking Neural Network??? (WTF!!!!!!!)
	I think this paper is outlier and i don't write literature review for this.	
	
	
	
	
	
	\newpage
	\bibliographystyle{IEEEtran}
	\bibliography{refs}
	
\end{document}
