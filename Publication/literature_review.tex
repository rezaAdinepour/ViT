\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}




\title{Literature Review for Ingur Thesis References}
\author{Reza}
\date{\today}

\begin{document}
	
	\maketitle
	\tableofcontents
		
	\chapter{BinaryViT \cite{le2023binaryvit}}
	
	\section{Introduction}
	The paper addresses the challenge of improving the performance of binary Vision Transformers (ViTs), a class of deep learning models used in computer vision. While ViTs have shown great potential, particularly when trained on large datasets, they suffer significant performance loss when binarized — a technique that reduces computational costs by converting model weights and activations into binary values. This performance drop is especially notable compared to convolutional neural networks (CNNs), which handle binarization more effectively.
	
	
	\section{Proposed Model}
	The paper identifies that the architecture of standard ViTs lacks key features present in CNNs, which allows CNNs to maintain higher representational capability even after binarization. To address this, the authors propose BinaryViT, a model that incorporates several features inspired by CNNs into the ViT architecture, without using convolutions. These enhancements include:
	
	\subsection{Global Average Pooling Layer}
	Replacing the token pooling layer with a global average pooling layer, which helps gather more information from input patches.
	
	\textbf{Previous Approach:} In standard ViTs, a token pooling layer is used before the classifier layer, which only takes into account information from the CLS token rather than considering all tokens in the input sequence.
	
	\textbf{Proposed Change:} The authors replace the token pooling layer with a global average pooling layer. This ensures that the model incorporates information from all input tokens (or patches), not just the CLS token. By doing so, the final classifier layer has more flexibility and can capture richer feature representations. This addition significantly increases the representational capability of the binary model by aggregating the information from all patches, which is crucial for improving accuracy in binary settings.
	
	
	\subsection{Multiple Pooling Branches}
	Introducing multiple pooling branches in each block to increase representational capability.
	Adding an affine transformation before each residual connection to balance the scales of different layers.
	
	\textbf{Previous Approach:} Traditional ViTs, and earlier works in binary ViTs, use simple feed-forward layers (FFNs) after attention layers with limited flexibility in processing features.
	
	
	\textbf{Proposed Change:} Inspired by CNNs, where convolutional layers capture different spatial information, the authors introduce multiple average pooling branches in each block. Each branch has different kernel sizes (e.g., 1x3, 3x1, 1x5, 5x1), allowing the model to process and aggregate spatial information in multiple directions. This change enhances the binary ViT’s ability to represent more complex information, without adding significant computational overhead.
	
	
	
	\subsection{Affine Transformation Before Residual Connections}
	Incorporating a pyramid structure to process high-resolution features early on and reduce them as the model goes deeper, increasing its flexibility and performance.
	
	
	\textbf{Previous Approach:} In ViTs, the scale of hidden states grows deeper in the network layers, often causing the residual branches to overwhelm the main branches, leading to a decrease in the model’s effectiveness. Binary CNNs, such as ResNet, use batch normalization before residual connections, which helps balance the scale of different layers and improves performance.
	
	
	\textbf{Proposed Change:} To counter the issue of overwhelming residual connections, the authors introduce an affine transformation before each residual addition in the ViT architecture. This technique is inspired by batch normalization in CNNs, which helps maintain a balance between the main and residual branches. The transformation prevents residual connections from dominating the main branches and allows the binary ViT to maintain better feature flow and representation through deeper layers.
	
	
	
	
	
	
	\subsection{Pyramid Structure}
	\textbf{Previous Approach:} Binary ViTs (like DeiT) typically use a fixed resolution for the feature maps throughout the network, unlike CNNs that progressively downsample the feature maps and increase the number of channels as the network goes deeper. In CNNs, this pyramid structure is important for capturing features at different resolutions and improving representational capacity.
	
	\textbf{Proposed Change:} The authors introduce a pyramid structure in the binary ViT. In this architecture, the feature map size progressively decreases (downsampling) while the hidden dimension (number of channels) increases as the network goes deeper. This mirrors the pyramid structure found in CNNs, allowing the model to capture features at high resolution in the early stages and focus on more abstract, lower-resolution features in the later stages. This significantly improves the model’s ability to handle complex visual tasks, especially when binarized.
	
	
	
	\subsection{Binary Fully-Connected Layers with Enhanced Attention}
	\textbf{Previous Approach:} Standard ViTs rely on attention mechanisms, where matrix multiplications for query, key, and value operations are computationally expensive and prone to significant performance drops when binarized.
	
	\textbf{Proposed Change:} In the proposed BinaryViT model, the authors optimize the binary attention mechanism by modifying how attention probabilities are calculated. They apply scaling factors and rounding techniques to improve the binary attention probability matrix’s accuracy, using methods inspired by prior works like ReActNet and Bi-RealNet in binary CNNs. This enhancement ensures that the binary ViT can more effectively process information during self-attention, resulting in better performance.
	
	
	
	\subsection{Distillation from Full-Precision Models}
	\textbf{Previous Approach:} Previous methods for binary ViTs did not consistently use teacher-student knowledge distillation methods to reduce the performance gap between binary and full-precision models.
	
	\textbf{Proposed Change:} The authors use a full-precision ViT model as a teacher to guide the training of the binary ViT. They distill knowledge by minimizing the soft cross-entropy loss between the binary student model’s logits and the full-precision teacher’s logits. While distillation techniques were used in some prior works, the authors tailor it specifically to improve binary ViT performance, focusing on logits rather than other components like attention scores or feed-forward outputs, which caused performance degradation in previous experiments.
	
	
	\section{Impact of the Changes}
	These architectural modifications collectively improve the performance of binary ViTs, making them competitive with binary CNNs. The proposed BinaryViT model achieves a significant performance boost on the ImageNet-1k dataset, outperforming earlier binary transformer models. By integrating CNN-inspired architectural features into ViTs, the authors have managed to retain the benefits of transformer models while reducing the computational cost and maintaining high accuracy in a binarized setting.
	
	These changes provide a more efficient and flexible architecture for tasks requiring high performance on resource-constrained devices such as smartphones and edge devices.
	
	
	
	\section{Results and Improvements}
	The results and improvements announced in the BinaryViT method, as detailed in the paper, demonstrate significant advancements in the performance of binary Vision Transformers (ViTs) compared to previous approaches. Below is a breakdown of the results and the improvements achieved by this method:
	
	\subsection{Performance Improvement on ImageNet-1k}
	The proposed BinaryViT was evaluated on the ImageNet-1k dataset, a standard benchmark for image classification. The model showed significant performance improvements over baseline binary ViTs and previous state-of-the-art (SOTA) binary models. The key results include:
	\begin{enumerate}
		\item
		\textbf{Baseline binary DeiT-S (previous work):} 48.5\% top-1 accuracy on ImageNet-1k.
		
		
		\item 
		\textbf{BinaryViT (proposed method):} Achieved \textbf{67.7\% top-1 accuracy} using the proposed enhancements, representing a large leap of \textbf{19.2\% improvement} over the baseline binary ViT (DeiT-S).
		
		\item 
		The modified BinaryViT architecture with full-precision patch embedding layers (BinaryViT*) achieved an even higher \textbf{70.6\% top-1 accuracy}, making it competitive with top binary CNNs like ReActNet.
	\end{enumerate}
	
	\subsection{Efficiency in Terms of Operations and Parameters}
	BinaryViT not only improves accuracy but also maintains computational efficiency, making it suitable for deployment on edge devices with limited resources. Key findings include:
	
	\begin{enumerate}
		\item 
		\textbf{Operations (OPs):} The proposed BinaryViT model performed fewer operations compared to many SOTA binary models. For example, BinaryViT had $0.79 \times 10^8$ operations compared to ReActNet’s $1.93 \times 10^8$ operations, making it nearly \textbf{2.5× more efficient}.
		
		\item 
		\textbf{Parameters:} BinaryViT contains around 22.6 million parameters, which is comparable to the baseline binary ViT (DeiT-S) but significantly less than other competitive models such as ReActNet (21.8 million parameters for ResNet-34 backbone and 29.3 million parameters for MobileNet backbone).
		
		\item 
		\textbf{FLOPs (Floating-Point Operations):} BinaryViT performed $0.19 \times 10^8$ FLOPs, much lower than ReActNet and other competing models, further highlighting its efficiency.
	\end{enumerate}
	
	
	
	\subsection{Comparisons with State-of-the-Art (SOTA) Binary Models}
	The BinaryViT method was directly compared with other leading binary models, and the results are as follows:
	
	\begin{enumerate}
		\item 
		ReActNet (ResNet-34 backbone): Achieved 67.5\% top-1 accuracy on ImageNet-1k with $1.93 \times 10^8$ operations and 21.8 million parameters.
		
		\item 
		BinaryViT: Matched ReActNet’s accuracy of 67.7\%, but with significantly fewer operations $(0.79 \times 10^8$ vs. $1.93 \times 10^8)$ and similar parameter count (22.6 million).
		
		\item 
		ReActNet (MobileNet backbone): Achieved 70.1\% top-1 accuracy, while BinaryViT* (with full-precision patch embedding layers) closely followed with 70.6\% top-1 accuracy and fewer operations, making it highly competitive.
	\end{enumerate}
	
	
	
	\subsection{Impact of Architectural Enhancements}
	The authors tested the impact of each architectural enhancement introduced in BinaryViT. The individual contributions to the model’s performance are detailed as follows:
	
	\begin{enumerate}
		\item 
		\textbf{Global Average Pooling:} Replacing token pooling with global average pooling increased the top-1 accuracy from \textbf{48.5\% to 56.4\%}, demonstrating the value of incorporating information from all input tokens.
		
		\item 
		\textbf{Multiple Pooling Branches:} Adding multi-branch average pooling layers further improved the accuracy to \textbf{60.2\%}, showing that this design helps to enrich the representational power of the model.
		
		\item 
		\textbf{Affine Transformations:} Introducing affine transformations before residual connections (to balance feature scales) increased accuracy to \textbf{61.8\%}.
		
		\item 
		\textbf{Pyramid Structure:} Implementing the pyramid structure, which mimics CNNs by processing higher-resolution features early on, provided the biggest improvement, bringing accuracy up to \textbf{67.7\%}.
	\end{enumerate}
	
	
	
	\subsection{Reduction in Computational Complexity}
	One of the key improvements announced by the authors is the ability of BinaryViT to reduce computational complexity without sacrificing performance:
	
	\begin{enumerate}
		\item 
		\textbf{Lower Bit-Operations (BOPs)}: BinaryViT achieved a balance between bit-operations and floating-point operations, outperforming other methods in terms of efficiency.
		
		\item 
		\textbf{Efficient Scaling:} The pyramid structure, multi-branch pooling, and affine transformations ensure that the model remains computationally efficient while handling large-scale image datasets like ImageNet-1k.
	\end{enumerate}
	
	
	\subsubsection{Comparison Between Full-Precision and Binary Versions}
	The authors demonstrated that the proposed BinaryViT model maintains performance close to its full-precision counterpart:
	
	\begin{enumerate}
		\item 
		The full-precision DeiT-S achieves \textbf{79.9\%} top-1 accuracy.
		
		\item 
		The binary version of BinaryViT achieved \textbf{70.6\%} accuracy, closing much of the gap between full-precision and binary ViT models.
	\end{enumerate}
	
	
	\section{Overall Improvements}
	\begin{enumerate}
		\item 
		\textbf{Significant performance boost:} BinaryViT improves the accuracy of binary ViTs by 19.2\% compared to the baseline, making it competitive with binary CNNs.
		
		\item 
		\textbf{Reduced operations and parameters:} BinaryViT achieves competitive performance with a lower computational cost, making it ideal for edge devices.
		
		\item 
		\textbf{Innovative architecture:} The introduction of CNN-inspired elements such as global average pooling, multiple branches, affine transformations, and pyramid structures enhances the performance of binary ViTs without introducing convolutions.
	\end{enumerate}
	
	
	
	
	
	
	
	\chapter{Vision Transformer for Small-Size Datasets \cite{DBLP:journals/corr/abs-2112-13492}}
	This paper focuses on improving the performance of Vision Transformers (ViTs) on small datasets. ViTs, which have shown remarkable success in large-scale datasets, often struggle with small datasets due to their weak locality inductive bias. This bias is critical in image classification tasks as it allows models to focus on local relationships between pixels, which CNNs do well but ViTs lack.
	
	The authors propose two main techniques to address this issue:
	
	\section{Shifted Patch Tokenization (SPT)}
	This method aims to improve the tokenization process by spatially shifting image patches in different directions before feeding them into the model. This shift increases the receptive field of each token, allowing the ViT to capture more spatial relationships between neighboring pixels, which enhances the model's ability to understand local features in an image.
	
	\subsection{Previous Approach:}
	Traditional Vision Transformers divide an input image into non-overlapping patches and treat each patch as a token, which is then fed into the transformer for processing. This method lacks spatial awareness between adjacent patches because the patches are non-overlapping. In CNNs, the use of convolutional filters ensures that neighboring pixels are processed together, allowing the network to capture local spatial information. However, ViTs, without such mechanisms, have limited capacity to capture local context.
	
	\subsection{Proposed Change:}
	The authors introduce Shifted Patch Tokenization (SPT), which enhances the spatial relationship between image patches. The core idea behind SPT is to spatially shift an image in multiple directions (up-left, up-right, down-left, down-right) before dividing it into patches. These shifted versions of the image are then concatenated with the original image and passed through the tokenization process. This results in a larger receptive field for each patch, enabling the model to capture more spatial relationships between neighboring pixels.
	
	\begin{enumerate}
		\item 
		\textbf{Impact:} SPT improves the model’s ability to understand local pixel interactions, which is particularly important for smaller datasets where capturing fine details is crucial. By increasing the locality inductive bias, the ViT performs more like a CNN in terms of capturing local information, while still leveraging the benefits of self-attention.
	\end{enumerate}
	
	
	
	
	
	\section{Locality Self-Attention (LSA)}
	This technique adjusts the attention mechanism in ViTs to focus more on local regions of an image. LSA uses two strategies: diagonal masking (removing the attention between a token and itself) and learnable temperature scaling (sharpening the attention score distribution). These adjustments prevent the attention from becoming too smooth, forcing it to focus more locally, thus boosting the model’s ability to differentiate between important regions in an image.
	
	\subsection{Previous Approach:}
	In standard ViTs, the self-attention mechanism evaluates the relationship between all tokens in an image. While this approach is effective for large datasets, it tends to be inefficient for small datasets because it results in a uniform distribution of attention across tokens. This means that ViTs often fail to focus on the most relevant tokens, especially in smaller images where local details matter more. Additionally, the attention scores tend to be smoothed due to the use of high temperatures in the softmax function, making it harder for the model to attend to important local regions.
	
	\subsection{Proposed Change:}
	The authors introduce Locality Self-Attention (LSA), which modifies the attention mechanism in two significant ways:
	
	\begin{enumerate}
		\item 
		\textbf{Diagonal Masking:} This method excludes self-tokens from the attention process. In standard attention mechanisms, tokens often pay too much attention to themselves (self-tokens). Diagonal masking forces the model to focus on relationships between different tokens rather than giving undue weight to each token itself.
		
		\item 
		\textbf{Learnable Temperature Scaling:} The authors propose adding a learnable temperature parameter to the softmax function, allowing the model to sharpen the attention distribution. A lower temperature sharpens the attention scores, helping the model focus on the most important tokens, particularly in the local regions of an image.
		
		\item 
		\textbf{Impact:} These two changes together reduce the tendency of ViTs to spread attention too broadly across the entire image. Instead, the attention becomes more focused on local regions, improving the ability of the model to recognize patterns and details within smaller datasets. LSA makes the attention mechanism more fine-tuned, thus improving performance on small-scale data.
	\end{enumerate}




	\section{Comparison to Other Data-Efficient ViTs}
	The paper compares the proposed SPT and LSA techniques to prior data-efficient ViT models, such as:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT (Data-efficient Image Transformer):} DeiT introduced techniques like knowledge distillation and data augmentations to make ViTs more efficient for training on mid-sized datasets like ImageNet. While effective, it still relies on large datasets and does not specifically address issues with small datasets.
		
		\item 
		\textbf{T2T-ViT (Tokens-to-Tokens ViT):} T2T-ViT introduced overlapping patches to improve the spatial relationship between patches. However, it did not fully solve the locality inductive bias issue as it only slightly increased the receptive field of the tokens.
		
		\item 
		\textbf{PiT (Pooling-based Vision Transformer):} PiT introduced a hierarchical pooling structure similar to CNNs to generate multi-scale features, allowing for better generalization on smaller datasets. However, it still does not effectively capture fine-grained local spatial information like SPT and LSA.
	\end{enumerate}
	
	In contrast, the SPT and LSA techniques specifically address the locality inductive bias in a more targeted way by increasing the receptive field during tokenization (SPT) and making attention more locally focused (LSA). These changes allow the proposed ViT to learn from small datasets effectively without relying on external large-scale pre-training, which was a limitation of previous models.
	
	
	
	\section{Efficiency vs. Performance Trade-offs}
	\subsection{Previous Models:}
	Many of the prior ViT-based models aimed to improve performance but often at the cost of computational efficiency. For example, DeiT used knowledge distillation, and T2T employed a complex overlapping tokenization method, both of which added computational overhead.
	
	
	\subsection{Proposed Model:}
	The proposed BinaryViT maintains competitive performance without a significant increase in computational cost. The SPT technique increases the receptive field without introducing convolutions or pooling layers, and LSA fine-tunes the attention mechanism with minimal additional parameters. As a result, the authors claim that BinaryViT improves accuracy on small datasets while maintaining acceptable overhead in terms of computational complexity.
	
	
	\section{Performance Gains}
	The experimental results in the paper show that the proposed BinaryViT model achieves substantial performance improvements over both the standard ViT and prior data-efficient ViTs when tested on small datasets like CIFAR-100, Tiny-ImageNet, and ImageNet. The model achieves these gains primarily due to its improved ability to capture local spatial information, a limitation that previous models struggled with.
	
	For example:
	\begin{enumerate}
		\item 
		In CIFAR-100, the use of SPT and LSA leads to an accuracy improvement of around 3-4\% compared to the baseline ViT model.
		
		\item 
		In Tiny-ImageNet, BinaryViT improves accuracy by up to 4.08\%, making it highly competitive with state-of-the-art CNNs on small datasets.
		
		\item 
		Even on a mid-sized dataset like ImageNet, the proposed changes result in a performance boost of 1.06\% to 1.60\%, demonstrating that the improvements are not limited to only small datasets.
	\end{enumerate}
	
	
	\section{Overall Impact of the Proposed Changes}
	The changes proposed by the authors—Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA)—represent significant architectural improvements that specifically address the limitations of Vision Transformers on small datasets. By increasing the locality inductive bias, these techniques make ViTs more efficient and effective at capturing the fine details that are crucial for tasks involving smaller datasets, bridging the gap between CNNs and transformers in this space.
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Performance Improvements on Small Datasets}
	The authors evaluated their methods on various small datasets, such as CIFAR-10, CIFAR-100, Tiny-ImageNet, and SVHN. They compared the performance of standard ViT models with and without the proposed SPT and LSA modules. The key findings are:
	
	\begin{enumerate}
		\item 
		\textbf{CIFAR-100:} The accuracy improved by up to 3.43\% for the CaiT model and 4.01\% for the PiT model when using SPT and LSA.
		
		\item 
		\textbf{Tiny-ImageNet:} ViTs saw significant performance boosts, with up to 4.08\% improvement for the Swin Transformer and 4.00\% improvement for the baseline ViT.
		
		\item 
		\textbf{SVHN and CIFAR-10:} Moderate improvements were observed, with a maximum gain of around 1-2\% for some models.
	\end{enumerate}
	These results highlight that the proposed methods effectively improve ViT performance on small datasets, where the original ViT architectures struggle.
	
	\subsection{Improvements in ImageNet Performance}
	While the methods were primarily designed for small datasets, they were also tested on the larger ImageNet dataset to verify if the improvements generalize to mid-sized data. The results show that the proposed methods also enhance ViT performance on ImageNet:
	
	\begin{enumerate}
		\item 
		\textbf{ViT:} Performance increased by 1.60\%, achieving a top-1 accuracy of 71.55\% (compared to 69.95\% for the baseline ViT).
		
		\item 
		\textbf{PiT: }Improved by 1.44\%, reaching 77.02\% accuracy.
		
		\item 
		\textbf{Swin Transformer:} Gained 1.06\% in accuracy, reaching 81.01\%.
	\end{enumerate}
	These results indicate that SPT and LSA can enhance ViTs even on larger datasets like ImageNet, although their primary benefit is seen in smaller datasets.
	
	
	\subsection{Efficiency and Computational Overhead}
	One of the key advantages of the proposed methods is their minimal computational overhead. Despite the performance improvements, the added complexity from SPT and LSA is modest:
	
	\begin{enumerate}
		\item 
		\textbf{Throughput:} The proposed methods cause only slight reductions in throughput. For example, the addition of SPT and LSA caused a 1.12\% latency overhead for the ViT model, and similar small increases for other models.
		
		\item 
		\textbf{FLOPs and Parameters:} The increase in FLOPs (Floating Point Operations) and parameters was minimal, ensuring that the models remain efficient and deployable, even with the added improvements in locality inductive bias.
	\end{enumerate}
	
	
	\subsection{Ablation Study Results}
	The authors conducted an ablation study to demonstrate the individual contributions of SPT and LSA:
	
	\begin{enumerate}
		\item 
		SPT (Shifted Patch Tokenization): Improved performance independently by +1.43\% in Tiny-ImageNet.
		
		\item 
		LSA (Locality Self-Attention): Provided an independent boost of +3.60\% in Tiny-ImageNet.
		
		\item 
		Combining SPT and LSA: When both methods were applied together, the performance improvement reached +4.00\% in Tiny-ImageNet, showing a strong synergy between the two methods.
	\end{enumerate}
	This shows that each technique effectively increases the model's ability to capture local details, and when used together, they yield even greater performance gains.
	
	
	
	\subsection{Qualitative Improvements}
	In addition to quantitative results, the authors provided qualitative visualizations of the ViT models’ attention maps. They compared the attention scores of final class tokens with and without the proposed methods:
	
	\begin{enumerate}
		\item 
		\textbf{Object Shapes:} When SPT and LSA were applied, the attention maps better captured the object shapes, focusing more on the relevant parts of the image, and avoiding excessive attention on background elements.
		
		\item 
		\textbf{Sharper Attention:} The learnable temperature scaling in LSA sharpened the attention distribution, leading to more focused and accurate attention on the target objects in images.
	\end{enumerate}
	These qualitative results visually demonstrate that the proposed changes help the model better understand the structure of the images, especially on smaller datasets where fine-grained details are essential.
	
	
	
	\subsection{Comparison with State-of-the-Art (SOTA) Models}
	The authors compared their proposed ViT models (with SPT and LSA) against several state-of-the-art (SOTA) models, including CNN-based models like ResNet and EfficientNet. The results showed that:
	
	\begin{enumerate}
		\item 
		\textbf{SL-CaiT:} Achieved better performance than ResNet and EfficientNet on most small datasets (except CIFAR-10).
		
		\item 
		\textbf{SL-Swin:} Provided comparable or better performance than CNNs while maintaining higher throughput.
	\end{enumerate}
	These comparisons highlight the ability of the modified ViTs to close the performance gap with CNNs on small datasets, a space where CNNs have traditionally outperformed transformers.
	
	
	
	
	\section{Key Takeaways:}
	\begin{enumerate}
		\item 
		\textbf{Substantial accuracy improvements:} The proposed SPT and LSA methods significantly enhance the performance of ViTs on small datasets, with gains of up to 4.08\% on Tiny-ImageNet and 3-4\% on CIFAR-100.
		
		\item 
		\textbf{Minimal computational overhead:} Despite the improvements, the increase in latency and computational cost is minimal, making these methods practical for deployment.
		
		\item 
		\textbf{Generalization to larger datasets:} While primarily aimed at small datasets, SPT and LSA also improve ViT performance on mid-sized datasets like ImageNet, with gains of up to 1.60\%.
		
		\item 
		\textbf{ViT competitiveness with CNNs:} The proposed methods make ViTs competitive with CNNs in small dataset tasks, both in terms of accuracy and computational efficiency.
	\end{enumerate}
	
	In conclusion, the results and improvements from the proposed methods mark a significant advancement for ViTs in handling small datasets, overcoming their limitations in local feature extraction, and making them competitive with traditional CNN architectures.

	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{How to train your ViT? Data, Augmentation,
		and Regularization in Vision Transformers \cite{DBLP:journals/corr/abs-2106-10270}}
		
		
	This paper, explores the best ways to train Vision Transformers (ViTs) effectively by balancing the use of data augmentation, regularization, model size, and available computational resources. ViTs are powerful models for computer vision tasks like image classification, but they tend to rely heavily on large datasets and regularization techniques to avoid overfitting. This article aims to provide practical insights for improving ViT performance, especially for practitioners with limited data and computational budgets.
	
	
	\section{Data Augmentation and Regularization ("AugReg")}
	The study shows that using proper data augmentation and regularization can yield models that perform as well as those trained on much larger datasets. By fine-tuning these techniques, smaller datasets can be used effectively, making the training process more efficient.
	
	\subsection{Previous Works:}
	Earlier studies on Vision Transformers, such as the original ViT paper, focused heavily on the need for large datasets like ImageNet-21k or JFT-300M to achieve competitive performance. The use of data augmentation and regularization was acknowledged, but the specific impact of these techniques on different dataset sizes, model configurations, and compute budgets was not systematically explored.
	
	\subsection{Proposed Changes}
	The authors of this paper shift the focus to a systematic study of how data \textbf{augmentation and regularization} can act as powerful tools to improve the performance of ViTs, even when the available dataset is smaller. The idea is that well-applied augmentation techniques (like Mixup and RandAugment) and regularization methods (such as dropout and stochastic depth) can compensate for the lack of large datasets, mimicking the effects of increasing the dataset size.
	
	
	This approach differs from previous work by providing empirical evidence showing that with carefully chosen augmentation and regularization settings, models can achieve results comparable to those trained on much larger datasets. This is particularly relevant for practitioners with limited access to massive datasets.
	
	
	
	
	
	
	\section{Trade-offs Between Data, Augmentation, and Compute Budget}
	 The article systematically investigates how the size of training data, the use of augmentation and regularization, and the compute budget interact. It demonstrates that well-designed regularization and augmentation strategies can mimic the effect of significantly increasing the dataset size.
	 
	 \subsection{Previous Works:}
	 Many earlier studies on ViTs, such as the DeiT (Data-efficient Image Transformers) work, emphasized the importance of using teacher-student distillation to enhance the performance of ViTs on smaller datasets. While this approach improved results, it added complexity to the training pipeline. Additionally, previous work often considered fixed trade-offs between model size and dataset size, without systematically exploring the effect of compute budget and regularization across a wide range of scenarios.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors go beyond distillation-based techniques and offer a more comprehensive investigation into the \textbf{interplay between model size, data size, and compute budget}. They conduct experiments across different ViT variants (from small to large models) and different dataset sizes (ImageNet-1k and ImageNet-21k) while systematically adjusting the amount of compute and AugReg techniques.
	 
	 
	 This approach offers a more nuanced understanding of how to balance \textbf{model complexity, data augmentation, and regularization} to achieve optimal performance under various constraints, helping practitioners make better decisions based on their available resources.
	 
	 
	 
	 
	 
	 
	 \section{Regularization Techniques and Their Impact}
	 \subsection{Previous Works:}
	 The role of regularization in ViTs was relatively underexplored in previous work, with most efforts focusing on training larger models on massive datasets. Dropout and stochastic depth were sometimes applied, but their effects were not systematically tested across different model sizes and dataset conditions.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors explore the use of \textbf{regularization techniques} like dropout and stochastic depth in greater detail. They find that regularization primarily benefits larger models when trained for extended periods, and actually harms performance in smaller models or when training on smaller datasets like ImageNet-21k. They also conduct ablation studies to identify the best settings for regularization, determining that a peak dropout/stochastic depth probability of 0.1 works best.
	 
	 
	 
	 This more detailed exploration of regularization sets their work apart by offering actionable insights into when and how to apply regularization effectively in ViTs, providing a deeper understanding of its benefits and drawbacks.
	 
	 
	 
	 
	 
	 
	 
	 
	 \section{Impact of Model Size}
	 Larger models tend to benefit more from regularization techniques, but this comes with the cost of requiring more training time and computational resources. Smaller models, on the other hand, might not benefit as much from regularization and could even suffer a loss in performance.
	 
	 \subsection{Previous Works:}
	 Earlier studies on ViTs often treated model size as a static factor, with larger models generally preferred when training on large datasets. However, there was little guidance on how to adapt model size based on the available compute or dataset size.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors provide specific \textbf{model size recommendations} based on their findings. They suggest that \textbf{larger patch sizes (e.g., 32×32)} are often more effective than reducing model size (e.g., using "Tiny" ViT models) when compute resources are limited. This means that instead of making the model smaller, increasing the patch size can help maintain performance without increasing the computational load significantly.
	 
	 This recommendation is based on a systematic analysis of throughput, model size, and patch size, providing a practical guide for selecting the right model configuration depending on the computational constraints.
	
	
	
	
	
	
	
	
	
	
	\section{Pre-training and Transfer Learning}
	The study finds that models pre-trained on larger datasets, like ImageNet-21k, perform better across a variety of tasks, including transfer learning. However, practitioners are advised to carefully choose augmentation and regularization settings to match their available compute budget and dataset size.
	
	\subsection{Previous Works:}
	Earlier ViT papers focused heavily on large-scale pre-training (e.g., JFT-300M) and emphasized the importance of pre-training on massive datasets for transfer learning. However, this left smaller organizations or researchers without access to such data at a disadvantage.
	
	
	\subsection{Proposed Changes:}
	The authors show that \textbf{transfer learning from pre-trained models on smaller datasets (like ImageNet-1k or ImageNet-21k)}, combined with strong AugReg, can yield results similar to those obtained from pre-training on massive datasets. They provide practical recommendations for selecting pre-trained models and fine-tuning them on specific tasks, which is particularly helpful for practitioners without access to large pre-training datasets.
	
	
	This change democratizes the use of ViTs, making them more accessible to a broader range of users and use cases, and helping users achieve competitive performance without requiring access to extremely large datasets for pre-training.
	
	
	\section{Practical Recommendations}
	The authors offer several practical guidelines, such as preferring data augmentation over regularization for smaller datasets, and choosing larger models with proper augmentation for the best results in a transfer learning setup.
	
	
	
	
	\section{Overall Impact of Changes}
	These changes collectively offer a more flexible and practical approach to training Vision Transformers, making them more applicable to real-world scenarios with constrained resources. The authors provide a comprehensive guide on how to balance data augmentation, regularization, and compute budget, allowing practitioners to achieve top-tier performance without relying on enormous datasets or computational resources. This is a significant shift from earlier ViT models, which focused primarily on large-scale data and heavy compute environments.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Training data-efficient image transformers \& distillation through attention \cite{DBLP:journals/corr/abs-2012-12877}}
	
	This paper focuses on making Vision Transformers (ViTs) more accessible and efficient by reducing their reliance on large datasets and expensive computing resources. Vision Transformers are highly effective for image classification, but their performance typically depends on massive datasets and extended training times on large infrastructure. This paper aims to train transformers effectively using only the ImageNet dataset on standard hardware, making these models more usable for a broader audience.
	
	
	
	\section{Data-Efficient Image Transformers (DeiT)}
	The authors propose a new training method for Vision Transformers, called DeiT, which allows these models to achieve high performance using only the ImageNet-1k dataset, without the need for the massive datasets like JFT-300M used in earlier ViT models. They demonstrate that it is possible to train ViTs efficiently on a single computer with 4 to 8 GPUs within just a few days, making them competitive with convolutional neural networks (CNNs) on standard benchmarks.
	
	\subsection{Previous Works:}
	The original ViT model by Dosovitskiy et al. (2020) showed exceptional performance, but it required training on extremely large datasets like JFT-300M, which contains 300 million labeled images. This makes the model less accessible for researchers and organizations with limited computational resources. The training of ViT models also demanded significant infrastructure, often involving many GPUs over long periods.
	
	\subsection{Proposed Changes:}
	The authors propose the DeiT (Data-efficient Image Transformer), a novel approach for training ViTs more efficiently. Key changes in the training strategy include:
	\begin{enumerate}
		\item 
		\textbf{Training on ImageNet-1k only:} The authors demonstrate that it is possible to train ViTs using only the 1.28 million images in the ImageNet-1k dataset, instead of relying on large-scale private datasets.
		
		\item 
		\textbf{Shorter Training Time:} The authors successfully trained their models on a single machine with 4 to 8 GPUs in less than three days, significantly reducing the computational cost and making it accessible to a wider range of users.
		
		\item 
		\textbf{Use of Repeated Augmentation:} Repeated augmentation was introduced to provide more data variations during training, allowing the model to generalize better on smaller datasets. This is crucial for data efficiency in training without relying on external datasets.
	\end{enumerate}
	By optimizing the training setup, the DeiT model becomes more practical for real-world use cases, where large-scale datasets and high-end computing resources are unavailable.
	
	
	
	
	
	
	
	\section{Distillation Through Attention}
	A unique contribution of this paper is the introduction of a distillation token, a new method that enables the student model (transformer) to learn from a teacher model (either a CNN or another transformer). This token is added to the transformer’s input and interacts with the class token during training. The distillation token improves the model's performance by helping it mimic the predictions of the teacher, allowing the transformer to learn more efficiently from the teacher’s inductive biases, especially when the teacher is a CNN.
	
	\subsection{Previous Works:}
	Knowledge distillation is a common technique used in CNN models, where a smaller "student" model learns from a larger "teacher" model, typically by mimicking the teacher's output (soft labels). Previous works used this approach for model compression and transfer learning, but it was not specifically adapted to the transformer architecture.
	
	
	\subsection{Proposed Changes:}
	The authors introduce a novel distillation mechanism specifically designed for transformers. The key innovation is the distillation token, which operates alongside the class token. Unlike the typical class token that learns from the ground truth labels, the distillation token learns from the teacher model’s predictions, allowing the student transformer to benefit from the teacher's inductive biases.
	
	\begin{enumerate}
		\item 
		\textbf{Interaction Through Attention:} The distillation token interacts with the other tokens through the self-attention mechanism, ensuring that the student model receives rich, token-level information from the teacher.
		
		\item 
		\textbf{Inductive Bias Transfer:} Interestingly, the paper shows that a CNN teacher (such as RegNet) transfers its inductive biases (such as convolutional feature learning) to the transformer through this attention-based distillation process, making the transformer perform better on image recognition tasks.
	\end{enumerate}
	This approach to distillation is a significant departure from earlier methods because it leverages the attention mechanism to integrate the teacher's guidance into the student's learning process, improving the model's performance.
	
	
	
	\section{Smaller and More Efficient Models (DeiT-S and DeiT-Ti)}
	\subsection{Previous Works:}
	The original ViT model introduced a large transformer architecture (ViT-B) that required massive datasets and significant computational resources to achieve competitive results. Smaller versions of ViTs had not been thoroughly explored, and the effectiveness of scaling down transformers while maintaining performance was not well established.
	
	
	\subsection{Proposed Changes:}
	The authors introduce two smaller versions of the ViT model in their DeiT approach:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-S (Small):} A smaller version with fewer parameters and heads, designed to be more efficient while still maintaining competitive performance.
		
		\item 
		\textbf{DeiT-Ti (Tiny):} An even smaller model, comparable to ResNet-18 in terms of parameter count, which is designed to be lightweight and fast while achieving solid accuracy on image classification tasks.
	\end{enumerate}
	These smaller models are trained using the same data-efficient strategies and distillation techniques as the larger DeiT-B model, making them highly competitive with traditional CNNs like ResNet. The development of these smaller models is a step forward in making transformers accessible for deployment on edge devices and environments with limited computational power.
	
	
	
	
	\section{Performance and Efficiency Gains}
	The proposed DeiT models achieve competitive results with state-of-the-art CNNs, reaching up to 85.2\% top-1 accuracy on ImageNet while requiring less compute time and fewer resources. The authors show that, with their distillation method, Vision Transformers can even outperform their teacher models in some cases, demonstrating the effectiveness of this distillation approach.
	
	
	\subsection{Previous Works:}
	Earlier Vision Transformer models (such as ViT-B and ViT-L) were known for their large computational overhead. Their training and inference speed were slower than state-of-the-art CNN models like EfficientNet and ResNet, limiting their usability in real-time or resource-constrained environments.
	
	
	\subsection{Proposed Changes:}
	The authors focus on improving the \textbf{throughput} (images processed per second) of DeiT models, ensuring that they are efficient in terms of both \textbf{accuracy} and \textbf{speed}. They show that:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-S} achieves higher throughput than many large CNNs like EfficientNet-B4 while maintaining competitive accuracy.
		
		\item 
		\textbf{The DeiT-B} model surpasses earlier ViTs trained on ImageNet-1k in terms of throughput, closing the gap between transformers and CNNs.
		
		\item 
		Through their distillation process, the authors achieve \textbf{better accuracy-to-throughput ratios} than standard CNNs, meaning that transformers can now offer high performance without compromising speed.
	\end{enumerate}
	
	
	
	\section{Transfer Learning and Generalization}
	The authors also explore the transfer learning capabilities of the DeiT models, showing that they perform well when fine-tuned on other popular datasets like CIFAR-10, CIFAR-100, and iNaturalist, further proving their generalization power.
	
	\subsection{Previous Works:}
	Vision Transformers, especially in their original form, struggled with transfer learning performance when fine-tuned on smaller datasets, often requiring extensive fine-tuning or retraining with external datasets. This was a significant limitation compared to CNNs, which have long been effective at transferring learned features across tasks and domains.
	
	
	\subsection{Proposed Changes:}
	The DeiT models are shown to \textbf{generalize well} to downstream tasks through transfer learning, performing competitively on popular datasets such as CIFAR-10, CIFAR-100, and iNaturalist. Key improvements include:
	
	\begin{enumerate}
		\item 
		\textbf{Fine-tuning at higher resolutions:} The authors successfully fine-tune DeiT models at different image resolutions, demonstrating that they can achieve high accuracy even when transferred to tasks that require different input sizes.
		
		\item 
		\textbf{Cross-dataset generalization:} DeiT models achieve competitive performance across various fine-grained classification tasks, proving their versatility beyond ImageNet. The model can be fine-tuned efficiently on smaller datasets, making it a more general-purpose model.
	\end{enumerate}
	This shows that DeiT not only excels at training from scratch but also performs strongly in transfer learning scenarios, making it more suitable for a broader range of tasks compared to previous ViT models.
	
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Competitive Performance with Smaller Datasets}
	The primary contribution of this work is that it shows Vision Transformers (ViTs) can achieve competitive results using only the ImageNet-1k dataset. Previous ViTs required massive datasets like JFT-300M for training. The authors achieve the following:
	\begin{enumerate}
		\item 
		\textbf{DeiT-B} (Base model) achieves \textbf{83.1\% top-1 accuracy} on ImageNet-1k, with no external data.
		
		\item 
		When fine-tuned at higher resolution (384×384), \textbf{DeiT-B} achieves \textbf{85.2\% top-1 accuracy}, surpassing many state-of-the-art CNN models like ResNet and EfficientNet.
	\end{enumerate}
	
	These results highlight that DeiT models can perform on par with models that rely on much larger datasets, making ViTs more accessible to the broader machine learning community.
	
	
	\subsection{Distillation Through Attention Enhances Model Performance}
	The introduction of the \textbf{distillation token} is a novel improvement in this work. This token allows the transformer model to learn from a teacher (often a CNN) in a more effective manner than traditional distillation methods. The benefits of this approach include:
	\begin{enumerate}
		\item 
		\textbf{DeiT-B with distillation (DeiT-B)} achieves \textbf{84.4\% top-1 accuracy} on ImageNet-1k, which is higher than the original ViT-B model (trained on larger datasets) and better than CNN models of similar size.
		
		\item 
		The distillation token outperforms standard soft and hard distillation techniques, showing that it is particularly effective in helping the transformer model adopt the inductive biases from a CNN teacher.
	\end{enumerate}
	This result demonstrates that transformers can benefit from distilling knowledge from CNNs, making the training process more efficient and effective.
	
	
	
	\subsection{Improved Throughput and Computational Efficiency}
	Another key improvement is the \textbf{throughput and computational efficiency} of the DeiT models. Compared to earlier Vision Transformers, the DeiT models:
	
	\begin{enumerate}
		\item 
		Achieve faster \textbf{throughput while maintaining high accuracy}. For example, DeiT-Tiny (DeiT-Ti), the smallest model, processes 2536 images per second, making it one of the fastest ViTs available.
		
		\item 
		\textbf{Outperform ViTs trained on larger datasets} in terms of the balance between accuracy and throughput. The DeiT models are competitive with CNNs like EfficientNet and ResNet in terms of both speed and accuracy.
	\end{enumerate}
	The improved throughput and reduced computational requirements make DeiT models more suitable for real-world applications, especially in environments with limited computational resources.
	
	
	
	
	\subsection{Smaller Models with Comparable Accuracy}
	The paper introduces smaller and more efficient variants of the DeiT model:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-S (Small model)} achieves \textbf{79.8\% top-1 accuracy} on ImageNet-1k while having fewer parameters than ResNet-50, making it a strong alternative to CNNs for image classification tasks.
		
		\item 
		\textbf{DeiT-Ti (Tiny model)}, a smaller version comparable to ResNet-18, achieves \textbf{72.2\% top-1 accuracy} on ImageNet-1k, showing that even small ViT models can be effective when trained efficiently.
	\end{enumerate}
	
	These smaller models provide flexibility in choosing the right trade-off between model size, speed, and accuracy, depending on the task at hand.
	
	
	
	
	\subsection{Transfer Learning and Generalization}
	The authors demonstrate that DeiT models generalize well to downstream tasks, achieving competitive performance in transfer learning scenarios. Key results include:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-B} \textbf{achieves 99.1\% accuracy on CIFAR-10, 91.4\% on CIFAR-100}, and \textbf{93.9\% on Stanford Cars}, which are common benchmarks for transfer learning tasks.
		
		\item 
		The models also perform well on fine-grained classification tasks and datasets like iNaturalist, Oxford-102 Flowers, and Stanford Cars.
	\end{enumerate}These results show that the DeiT models are not only effective for large-scale classification tasks like ImageNet but also excel in transfer learning, making them versatile for different types of datasets and tasks.
	
	
	
	
	\subsection{Training Time Reduction}
	One of the critical improvements in this method is the reduced training time:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT-B} is trained on \textbf{a single machine with 4 to 8 GPUs} in just three days, compared to the weeks or even months of training required for other ViT models on large datasets.
		
		\item 
		The fine-tuning of DeiT models at higher resolutions is also efficient, taking only around 20 hours on an 8-GPU machine for 25 epochs.
	\end{enumerate}
	This reduction in training time, combined with efficient throughput, makes DeiT models more practical and accessible for organizations with limited resources.
	
	
	
	\subsection{Distillation from CNNs Is More Effective than from Transformers}
	A surprising finding in the paper is that \textbf{transformers learn better when the teacher is a CNN rather than another transformer}. The authors observed that:
	
	\begin{enumerate}
		\item 
		When using a \textbf{RegNetY CNN} as the teacher, the student DeiT model outperforms models distilled from transformer teachers.
		
		\item 
		The distillation process allows the transformer to learn important inductive biases from the CNN, improving its ability to capture spatial and hierarchical information in images.
	\end{enumerate}
	
	This result suggests that CNNs can still play a valuable role in improving transformer models through distillation, leveraging the strengths of both architectures.
	
	
	
	\section{Overall Improvements}
	\begin{enumerate}
		\item 
		\textbf{Data Efficiency:} The DeiT models reduce the need for large datasets, showing that ViTs can perform well with ImageNet-1k only.
		
		\item 
		\textbf{Distillation Innovation:} The novel distillation token mechanism leads to significant performance gains, especially when CNNs are used as teachers.
		
		\item 
		\textbf{Improved Throughput:} DeiT models achieve a better balance between accuracy and throughput, making them faster and more efficient for real-time applications.
		
		\item 
		\textbf{Smaller, Faster Models:} The introduction of DeiT-S and DeiT-Ti offers more flexibility with smaller models that perform competitively.
		
		\item 
		\textbf{Transfer Learning Success:} DeiT models generalize well across various downstream tasks, proving to be versatile for different datasets.
	\end{enumerate}
	
	
	

	
	
	
	
	
	
	
	
	
	
	
	\chapter{s}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\bibliographystyle{IEEEtran}
	\bibliography{refs} % Assuming your bibliography file is named bib.bib
	
\end{document}
