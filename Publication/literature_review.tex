\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}




\title{Literature Review for Ingur Thesis References}
\author{Reza}
\date{\today}

\begin{document}
	
	\maketitle
	\tableofcontents
		
	\chapter{BinaryViT \cite{le2023binaryvit}}
	
	\section{Introduction}
	The paper addresses the challenge of improving the performance of binary Vision Transformers (ViTs), a class of deep learning models used in computer vision. While ViTs have shown great potential, particularly when trained on large datasets, they suffer significant performance loss when binarized — a technique that reduces computational costs by converting model weights and activations into binary values. This performance drop is especially notable compared to convolutional neural networks (CNNs), which handle binarization more effectively.
	
	
	\section{Proposed Model}
	The paper identifies that the architecture of standard ViTs lacks key features present in CNNs, which allows CNNs to maintain higher representational capability even after binarization. To address this, the authors propose BinaryViT, a model that incorporates several features inspired by CNNs into the ViT architecture, without using convolutions. These enhancements include:
	
	\subsection{Global Average Pooling Layer}
	Replacing the token pooling layer with a global average pooling layer, which helps gather more information from input patches.
	
	\textbf{Previous Approach:} In standard ViTs, a token pooling layer is used before the classifier layer, which only takes into account information from the CLS token rather than considering all tokens in the input sequence.
	
	\textbf{Proposed Change:} The authors replace the token pooling layer with a global average pooling layer. This ensures that the model incorporates information from all input tokens (or patches), not just the CLS token. By doing so, the final classifier layer has more flexibility and can capture richer feature representations. This addition significantly increases the representational capability of the binary model by aggregating the information from all patches, which is crucial for improving accuracy in binary settings.
	
	
	\subsection{Multiple Pooling Branches}
	Introducing multiple pooling branches in each block to increase representational capability.
	Adding an affine transformation before each residual connection to balance the scales of different layers.
	
	\textbf{Previous Approach:} Traditional ViTs, and earlier works in binary ViTs, use simple feed-forward layers (FFNs) after attention layers with limited flexibility in processing features.
	
	
	\textbf{Proposed Change:} Inspired by CNNs, where convolutional layers capture different spatial information, the authors introduce multiple average pooling branches in each block. Each branch has different kernel sizes (e.g., 1x3, 3x1, 1x5, 5x1), allowing the model to process and aggregate spatial information in multiple directions. This change enhances the binary ViT’s ability to represent more complex information, without adding significant computational overhead.
	
	
	
	\subsection{Affine Transformation Before Residual Connections}
	Incorporating a pyramid structure to process high-resolution features early on and reduce them as the model goes deeper, increasing its flexibility and performance.
	
	
	\textbf{Previous Approach:} In ViTs, the scale of hidden states grows deeper in the network layers, often causing the residual branches to overwhelm the main branches, leading to a decrease in the model’s effectiveness. Binary CNNs, such as ResNet, use batch normalization before residual connections, which helps balance the scale of different layers and improves performance.
	
	
	\textbf{Proposed Change:} To counter the issue of overwhelming residual connections, the authors introduce an affine transformation before each residual addition in the ViT architecture. This technique is inspired by batch normalization in CNNs, which helps maintain a balance between the main and residual branches. The transformation prevents residual connections from dominating the main branches and allows the binary ViT to maintain better feature flow and representation through deeper layers.
	
	
	
	
	
	
	\subsection{Pyramid Structure}
	\textbf{Previous Approach:} Binary ViTs (like DeiT) typically use a fixed resolution for the feature maps throughout the network, unlike CNNs that progressively downsample the feature maps and increase the number of channels as the network goes deeper. In CNNs, this pyramid structure is important for capturing features at different resolutions and improving representational capacity.
	
	\textbf{Proposed Change:} The authors introduce a pyramid structure in the binary ViT. In this architecture, the feature map size progressively decreases (downsampling) while the hidden dimension (number of channels) increases as the network goes deeper. This mirrors the pyramid structure found in CNNs, allowing the model to capture features at high resolution in the early stages and focus on more abstract, lower-resolution features in the later stages. This significantly improves the model’s ability to handle complex visual tasks, especially when binarized.
	
	
	
	\subsection{Binary Fully-Connected Layers with Enhanced Attention}
	\textbf{Previous Approach:} Standard ViTs rely on attention mechanisms, where matrix multiplications for query, key, and value operations are computationally expensive and prone to significant performance drops when binarized.
	
	\textbf{Proposed Change:} In the proposed BinaryViT model, the authors optimize the binary attention mechanism by modifying how attention probabilities are calculated. They apply scaling factors and rounding techniques to improve the binary attention probability matrix’s accuracy, using methods inspired by prior works like ReActNet and Bi-RealNet in binary CNNs. This enhancement ensures that the binary ViT can more effectively process information during self-attention, resulting in better performance.
	
	
	
	\subsection{Distillation from Full-Precision Models}
	\textbf{Previous Approach:} Previous methods for binary ViTs did not consistently use teacher-student knowledge distillation methods to reduce the performance gap between binary and full-precision models.
	
	\textbf{Proposed Change:} The authors use a full-precision ViT model as a teacher to guide the training of the binary ViT. They distill knowledge by minimizing the soft cross-entropy loss between the binary student model’s logits and the full-precision teacher’s logits. While distillation techniques were used in some prior works, the authors tailor it specifically to improve binary ViT performance, focusing on logits rather than other components like attention scores or feed-forward outputs, which caused performance degradation in previous experiments.
	
	
	\section{Impact of the Changes}
	These architectural modifications collectively improve the performance of binary ViTs, making them competitive with binary CNNs. The proposed BinaryViT model achieves a significant performance boost on the ImageNet-1k dataset, outperforming earlier binary transformer models. By integrating CNN-inspired architectural features into ViTs, the authors have managed to retain the benefits of transformer models while reducing the computational cost and maintaining high accuracy in a binarized setting.
	
	These changes provide a more efficient and flexible architecture for tasks requiring high performance on resource-constrained devices such as smartphones and edge devices.
	
	
	
	\section{Results and Improvements}
	The results and improvements announced in the BinaryViT method, as detailed in the paper, demonstrate significant advancements in the performance of binary Vision Transformers (ViTs) compared to previous approaches. Below is a breakdown of the results and the improvements achieved by this method:
	
	\subsection{Performance Improvement on ImageNet-1k}
	The proposed BinaryViT was evaluated on the ImageNet-1k dataset, a standard benchmark for image classification. The model showed significant performance improvements over baseline binary ViTs and previous state-of-the-art (SOTA) binary models. The key results include:
	\begin{enumerate}
		\item
		\textbf{Baseline binary DeiT-S (previous work):} 48.5\% top-1 accuracy on ImageNet-1k.
		
		
		\item 
		\textbf{BinaryViT (proposed method):} Achieved \textbf{67.7\% top-1 accuracy} using the proposed enhancements, representing a large leap of \textbf{19.2\% improvement} over the baseline binary ViT (DeiT-S).
		
		\item 
		The modified BinaryViT architecture with full-precision patch embedding layers (BinaryViT*) achieved an even higher \textbf{70.6\% top-1 accuracy}, making it competitive with top binary CNNs like ReActNet.
	\end{enumerate}
	
	\subsection{Efficiency in Terms of Operations and Parameters}
	BinaryViT not only improves accuracy but also maintains computational efficiency, making it suitable for deployment on edge devices with limited resources. Key findings include:
	
	\begin{enumerate}
		\item 
		\textbf{Operations (OPs):} The proposed BinaryViT model performed fewer operations compared to many SOTA binary models. For example, BinaryViT had $0.79 \times 10^8$ operations compared to ReActNet’s $1.93 \times 10^8$ operations, making it nearly \textbf{2.5× more efficient}.
		
		\item 
		\textbf{Parameters:} BinaryViT contains around 22.6 million parameters, which is comparable to the baseline binary ViT (DeiT-S) but significantly less than other competitive models such as ReActNet (21.8 million parameters for ResNet-34 backbone and 29.3 million parameters for MobileNet backbone).
		
		\item 
		\textbf{FLOPs (Floating-Point Operations):} BinaryViT performed $0.19 \times 10^8$ FLOPs, much lower than ReActNet and other competing models, further highlighting its efficiency.
	\end{enumerate}
	
	
	
	\subsection{Comparisons with State-of-the-Art (SOTA) Binary Models}
	The BinaryViT method was directly compared with other leading binary models, and the results are as follows:
	
	\begin{enumerate}
		\item 
		ReActNet (ResNet-34 backbone): Achieved 67.5\% top-1 accuracy on ImageNet-1k with $1.93 \times 10^8$ operations and 21.8 million parameters.
		
		\item 
		BinaryViT: Matched ReActNet’s accuracy of 67.7\%, but with significantly fewer operations $(0.79 \times 10^8$ vs. $1.93 \times 10^8)$ and similar parameter count (22.6 million).
		
		\item 
		ReActNet (MobileNet backbone): Achieved 70.1\% top-1 accuracy, while BinaryViT* (with full-precision patch embedding layers) closely followed with 70.6\% top-1 accuracy and fewer operations, making it highly competitive.
	\end{enumerate}
	
	
	
	\subsection{Impact of Architectural Enhancements}
	The authors tested the impact of each architectural enhancement introduced in BinaryViT. The individual contributions to the model’s performance are detailed as follows:
	
	\begin{enumerate}
		\item 
		\textbf{Global Average Pooling:} Replacing token pooling with global average pooling increased the top-1 accuracy from \textbf{48.5\% to 56.4\%}, demonstrating the value of incorporating information from all input tokens.
		
		\item 
		\textbf{Multiple Pooling Branches:} Adding multi-branch average pooling layers further improved the accuracy to \textbf{60.2\%}, showing that this design helps to enrich the representational power of the model.
		
		\item 
		\textbf{Affine Transformations:} Introducing affine transformations before residual connections (to balance feature scales) increased accuracy to \textbf{61.8\%}.
		
		\item 
		\textbf{Pyramid Structure:} Implementing the pyramid structure, which mimics CNNs by processing higher-resolution features early on, provided the biggest improvement, bringing accuracy up to \textbf{67.7\%}.
	\end{enumerate}
	
	
	
	\subsection{Reduction in Computational Complexity}
	One of the key improvements announced by the authors is the ability of BinaryViT to reduce computational complexity without sacrificing performance:
	
	\begin{enumerate}
		\item 
		\textbf{Lower Bit-Operations (BOPs)}: BinaryViT achieved a balance between bit-operations and floating-point operations, outperforming other methods in terms of efficiency.
		
		\item 
		\textbf{Efficient Scaling:} The pyramid structure, multi-branch pooling, and affine transformations ensure that the model remains computationally efficient while handling large-scale image datasets like ImageNet-1k.
	\end{enumerate}
	
	
	\subsubsection{Comparison Between Full-Precision and Binary Versions}
	The authors demonstrated that the proposed BinaryViT model maintains performance close to its full-precision counterpart:
	
	\begin{enumerate}
		\item 
		The full-precision DeiT-S achieves \textbf{79.9\%} top-1 accuracy.
		
		\item 
		The binary version of BinaryViT achieved \textbf{70.6\%} accuracy, closing much of the gap between full-precision and binary ViT models.
	\end{enumerate}
	
	
	\section{Overall Improvements}
	\begin{enumerate}
		\item 
		\textbf{Significant performance boost:} BinaryViT improves the accuracy of binary ViTs by 19.2\% compared to the baseline, making it competitive with binary CNNs.
		
		\item 
		\textbf{Reduced operations and parameters:} BinaryViT achieves competitive performance with a lower computational cost, making it ideal for edge devices.
		
		\item 
		\textbf{Innovative architecture:} The introduction of CNN-inspired elements such as global average pooling, multiple branches, affine transformations, and pyramid structures enhances the performance of binary ViTs without introducing convolutions.
	\end{enumerate}
	
	
	
	
	
	
	
	\chapter{Vision Transformer for Small-Size Datasets \cite{DBLP:journals/corr/abs-2112-13492}}
	This paper focuses on improving the performance of Vision Transformers (ViTs) on small datasets. ViTs, which have shown remarkable success in large-scale datasets, often struggle with small datasets due to their weak locality inductive bias. This bias is critical in image classification tasks as it allows models to focus on local relationships between pixels, which CNNs do well but ViTs lack.
	
	The authors propose two main techniques to address this issue:
	
	\section{Shifted Patch Tokenization (SPT)}
	This method aims to improve the tokenization process by spatially shifting image patches in different directions before feeding them into the model. This shift increases the receptive field of each token, allowing the ViT to capture more spatial relationships between neighboring pixels, which enhances the model's ability to understand local features in an image.
	
	\subsection{Previous Approach:}
	Traditional Vision Transformers divide an input image into non-overlapping patches and treat each patch as a token, which is then fed into the transformer for processing. This method lacks spatial awareness between adjacent patches because the patches are non-overlapping. In CNNs, the use of convolutional filters ensures that neighboring pixels are processed together, allowing the network to capture local spatial information. However, ViTs, without such mechanisms, have limited capacity to capture local context.
	
	\subsection{Proposed Change:}
	The authors introduce Shifted Patch Tokenization (SPT), which enhances the spatial relationship between image patches. The core idea behind SPT is to spatially shift an image in multiple directions (up-left, up-right, down-left, down-right) before dividing it into patches. These shifted versions of the image are then concatenated with the original image and passed through the tokenization process. This results in a larger receptive field for each patch, enabling the model to capture more spatial relationships between neighboring pixels.
	
	\begin{enumerate}
		\item 
		\textbf{Impact:} SPT improves the model’s ability to understand local pixel interactions, which is particularly important for smaller datasets where capturing fine details is crucial. By increasing the locality inductive bias, the ViT performs more like a CNN in terms of capturing local information, while still leveraging the benefits of self-attention.
	\end{enumerate}
	
	
	
	
	
	\section{Locality Self-Attention (LSA)}
	This technique adjusts the attention mechanism in ViTs to focus more on local regions of an image. LSA uses two strategies: diagonal masking (removing the attention between a token and itself) and learnable temperature scaling (sharpening the attention score distribution). These adjustments prevent the attention from becoming too smooth, forcing it to focus more locally, thus boosting the model’s ability to differentiate between important regions in an image.
	
	\subsection{Previous Approach:}
	In standard ViTs, the self-attention mechanism evaluates the relationship between all tokens in an image. While this approach is effective for large datasets, it tends to be inefficient for small datasets because it results in a uniform distribution of attention across tokens. This means that ViTs often fail to focus on the most relevant tokens, especially in smaller images where local details matter more. Additionally, the attention scores tend to be smoothed due to the use of high temperatures in the softmax function, making it harder for the model to attend to important local regions.
	
	\subsection{Proposed Change:}
	The authors introduce Locality Self-Attention (LSA), which modifies the attention mechanism in two significant ways:
	
	\begin{enumerate}
		\item 
		\textbf{Diagonal Masking:} This method excludes self-tokens from the attention process. In standard attention mechanisms, tokens often pay too much attention to themselves (self-tokens). Diagonal masking forces the model to focus on relationships between different tokens rather than giving undue weight to each token itself.
		
		\item 
		\textbf{Learnable Temperature Scaling:} The authors propose adding a learnable temperature parameter to the softmax function, allowing the model to sharpen the attention distribution. A lower temperature sharpens the attention scores, helping the model focus on the most important tokens, particularly in the local regions of an image.
		
		\item 
		\textbf{Impact:} These two changes together reduce the tendency of ViTs to spread attention too broadly across the entire image. Instead, the attention becomes more focused on local regions, improving the ability of the model to recognize patterns and details within smaller datasets. LSA makes the attention mechanism more fine-tuned, thus improving performance on small-scale data.
	\end{enumerate}




	\section{Comparison to Other Data-Efficient ViTs}
	The paper compares the proposed SPT and LSA techniques to prior data-efficient ViT models, such as:
	
	\begin{enumerate}
		\item 
		\textbf{DeiT (Data-efficient Image Transformer):} DeiT introduced techniques like knowledge distillation and data augmentations to make ViTs more efficient for training on mid-sized datasets like ImageNet. While effective, it still relies on large datasets and does not specifically address issues with small datasets.
		
		\item 
		\textbf{T2T-ViT (Tokens-to-Tokens ViT):} T2T-ViT introduced overlapping patches to improve the spatial relationship between patches. However, it did not fully solve the locality inductive bias issue as it only slightly increased the receptive field of the tokens.
		
		\item 
		\textbf{PiT (Pooling-based Vision Transformer):} PiT introduced a hierarchical pooling structure similar to CNNs to generate multi-scale features, allowing for better generalization on smaller datasets. However, it still does not effectively capture fine-grained local spatial information like SPT and LSA.
	\end{enumerate}
	
	In contrast, the SPT and LSA techniques specifically address the locality inductive bias in a more targeted way by increasing the receptive field during tokenization (SPT) and making attention more locally focused (LSA). These changes allow the proposed ViT to learn from small datasets effectively without relying on external large-scale pre-training, which was a limitation of previous models.
	
	
	
	\section{Efficiency vs. Performance Trade-offs}
	\subsection{Previous Models:}
	Many of the prior ViT-based models aimed to improve performance but often at the cost of computational efficiency. For example, DeiT used knowledge distillation, and T2T employed a complex overlapping tokenization method, both of which added computational overhead.
	
	
	\subsection{Proposed Model:}
	The proposed BinaryViT maintains competitive performance without a significant increase in computational cost. The SPT technique increases the receptive field without introducing convolutions or pooling layers, and LSA fine-tunes the attention mechanism with minimal additional parameters. As a result, the authors claim that BinaryViT improves accuracy on small datasets while maintaining acceptable overhead in terms of computational complexity.
	
	
	\section{Performance Gains}
	The experimental results in the paper show that the proposed BinaryViT model achieves substantial performance improvements over both the standard ViT and prior data-efficient ViTs when tested on small datasets like CIFAR-100, Tiny-ImageNet, and ImageNet. The model achieves these gains primarily due to its improved ability to capture local spatial information, a limitation that previous models struggled with.
	
	For example:
	\begin{enumerate}
		\item 
		In CIFAR-100, the use of SPT and LSA leads to an accuracy improvement of around 3-4\% compared to the baseline ViT model.
		
		\item 
		In Tiny-ImageNet, BinaryViT improves accuracy by up to 4.08\%, making it highly competitive with state-of-the-art CNNs on small datasets.
		
		\item 
		Even on a mid-sized dataset like ImageNet, the proposed changes result in a performance boost of 1.06\% to 1.60\%, demonstrating that the improvements are not limited to only small datasets.
	\end{enumerate}
	
	
	\section{Overall Impact of the Proposed Changes}
	The changes proposed by the authors—Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA)—represent significant architectural improvements that specifically address the limitations of Vision Transformers on small datasets. By increasing the locality inductive bias, these techniques make ViTs more efficient and effective at capturing the fine details that are crucial for tasks involving smaller datasets, bridging the gap between CNNs and transformers in this space.
	
	
	
	
	
	\section{Results and Improvements}
	\subsection{Performance Improvements on Small Datasets}
	The authors evaluated their methods on various small datasets, such as CIFAR-10, CIFAR-100, Tiny-ImageNet, and SVHN. They compared the performance of standard ViT models with and without the proposed SPT and LSA modules. The key findings are:
	
	\begin{enumerate}
		\item 
		\textbf{CIFAR-100:} The accuracy improved by up to 3.43\% for the CaiT model and 4.01\% for the PiT model when using SPT and LSA.
		
		\item 
		\textbf{Tiny-ImageNet:} ViTs saw significant performance boosts, with up to 4.08\% improvement for the Swin Transformer and 4.00\% improvement for the baseline ViT.
		
		\item 
		\textbf{SVHN and CIFAR-10:} Moderate improvements were observed, with a maximum gain of around 1-2\% for some models.
	\end{enumerate}
	These results highlight that the proposed methods effectively improve ViT performance on small datasets, where the original ViT architectures struggle.
	
	\subsection{Improvements in ImageNet Performance}
	While the methods were primarily designed for small datasets, they were also tested on the larger ImageNet dataset to verify if the improvements generalize to mid-sized data. The results show that the proposed methods also enhance ViT performance on ImageNet:
	
	\begin{enumerate}
		\item 
		\textbf{ViT:} Performance increased by 1.60\%, achieving a top-1 accuracy of 71.55\% (compared to 69.95\% for the baseline ViT).
		
		\item 
		\textbf{PiT: }Improved by 1.44\%, reaching 77.02\% accuracy.
		
		\item 
		\textbf{Swin Transformer:} Gained 1.06\% in accuracy, reaching 81.01\%.
	\end{enumerate}
	These results indicate that SPT and LSA can enhance ViTs even on larger datasets like ImageNet, although their primary benefit is seen in smaller datasets.
	
	
	\subsection{Efficiency and Computational Overhead}
	One of the key advantages of the proposed methods is their minimal computational overhead. Despite the performance improvements, the added complexity from SPT and LSA is modest:
	
	\begin{enumerate}
		\item 
		\textbf{Throughput:} The proposed methods cause only slight reductions in throughput. For example, the addition of SPT and LSA caused a 1.12\% latency overhead for the ViT model, and similar small increases for other models.
		
		\item 
		\textbf{FLOPs and Parameters:} The increase in FLOPs (Floating Point Operations) and parameters was minimal, ensuring that the models remain efficient and deployable, even with the added improvements in locality inductive bias.
	\end{enumerate}
	
	
	\subsection{Ablation Study Results}
	The authors conducted an ablation study to demonstrate the individual contributions of SPT and LSA:
	
	\begin{enumerate}
		\item 
		SPT (Shifted Patch Tokenization): Improved performance independently by +1.43\% in Tiny-ImageNet.
		
		\item 
		LSA (Locality Self-Attention): Provided an independent boost of +3.60\% in Tiny-ImageNet.
		
		\item 
		Combining SPT and LSA: When both methods were applied together, the performance improvement reached +4.00\% in Tiny-ImageNet, showing a strong synergy between the two methods.
	\end{enumerate}
	This shows that each technique effectively increases the model's ability to capture local details, and when used together, they yield even greater performance gains.
	
	
	
	\subsection{Qualitative Improvements}
	In addition to quantitative results, the authors provided qualitative visualizations of the ViT models’ attention maps. They compared the attention scores of final class tokens with and without the proposed methods:
	
	\begin{enumerate}
		\item 
		\textbf{Object Shapes:} When SPT and LSA were applied, the attention maps better captured the object shapes, focusing more on the relevant parts of the image, and avoiding excessive attention on background elements.
		
		\item 
		\textbf{Sharper Attention:} The learnable temperature scaling in LSA sharpened the attention distribution, leading to more focused and accurate attention on the target objects in images.
	\end{enumerate}
	These qualitative results visually demonstrate that the proposed changes help the model better understand the structure of the images, especially on smaller datasets where fine-grained details are essential.
	
	
	
	\subsection{Comparison with State-of-the-Art (SOTA) Models}
	The authors compared their proposed ViT models (with SPT and LSA) against several state-of-the-art (SOTA) models, including CNN-based models like ResNet and EfficientNet. The results showed that:
	
	\begin{enumerate}
		\item 
		\textbf{SL-CaiT:} Achieved better performance than ResNet and EfficientNet on most small datasets (except CIFAR-10).
		
		\item 
		\textbf{SL-Swin:} Provided comparable or better performance than CNNs while maintaining higher throughput.
	\end{enumerate}
	These comparisons highlight the ability of the modified ViTs to close the performance gap with CNNs on small datasets, a space where CNNs have traditionally outperformed transformers.
	
	
	
	
	\section{Key Takeaways:}
	\begin{enumerate}
		\item 
		\textbf{Substantial accuracy improvements:} The proposed SPT and LSA methods significantly enhance the performance of ViTs on small datasets, with gains of up to 4.08\% on Tiny-ImageNet and 3-4\% on CIFAR-100.
		
		\item 
		\textbf{Minimal computational overhead:} Despite the improvements, the increase in latency and computational cost is minimal, making these methods practical for deployment.
		
		\item 
		\textbf{Generalization to larger datasets:} While primarily aimed at small datasets, SPT and LSA also improve ViT performance on mid-sized datasets like ImageNet, with gains of up to 1.60\%.
		
		\item 
		\textbf{ViT competitiveness with CNNs:} The proposed methods make ViTs competitive with CNNs in small dataset tasks, both in terms of accuracy and computational efficiency.
	\end{enumerate}
	
	In conclusion, the results and improvements from the proposed methods mark a significant advancement for ViTs in handling small datasets, overcoming their limitations in local feature extraction, and making them competitive with traditional CNN architectures.

	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{How to train your ViT? Data, Augmentation,
		and Regularization in Vision Transformers \cite{DBLP:journals/corr/abs-2106-10270}}
		
		
	This paper, explores the best ways to train Vision Transformers (ViTs) effectively by balancing the use of data augmentation, regularization, model size, and available computational resources. ViTs are powerful models for computer vision tasks like image classification, but they tend to rely heavily on large datasets and regularization techniques to avoid overfitting. This article aims to provide practical insights for improving ViT performance, especially for practitioners with limited data and computational budgets.
	
	
	\section{Data Augmentation and Regularization ("AugReg")}
	The study shows that using proper data augmentation and regularization can yield models that perform as well as those trained on much larger datasets. By fine-tuning these techniques, smaller datasets can be used effectively, making the training process more efficient.
	
	\subsection{Previous Works:}
	Earlier studies on Vision Transformers, such as the original ViT paper, focused heavily on the need for large datasets like ImageNet-21k or JFT-300M to achieve competitive performance. The use of data augmentation and regularization was acknowledged, but the specific impact of these techniques on different dataset sizes, model configurations, and compute budgets was not systematically explored.
	
	\subsection{Proposed Changes}
	The authors of this paper shift the focus to a systematic study of how data \textbf{augmentation and regularization} can act as powerful tools to improve the performance of ViTs, even when the available dataset is smaller. The idea is that well-applied augmentation techniques (like Mixup and RandAugment) and regularization methods (such as dropout and stochastic depth) can compensate for the lack of large datasets, mimicking the effects of increasing the dataset size.
	
	
	This approach differs from previous work by providing empirical evidence showing that with carefully chosen augmentation and regularization settings, models can achieve results comparable to those trained on much larger datasets. This is particularly relevant for practitioners with limited access to massive datasets.
	
	
	
	
	
	
	\section{Trade-offs Between Data, Augmentation, and Compute Budget}
	 The article systematically investigates how the size of training data, the use of augmentation and regularization, and the compute budget interact. It demonstrates that well-designed regularization and augmentation strategies can mimic the effect of significantly increasing the dataset size.
	 
	 \subsection{Previous Works:}
	 Many earlier studies on ViTs, such as the DeiT (Data-efficient Image Transformers) work, emphasized the importance of using teacher-student distillation to enhance the performance of ViTs on smaller datasets. While this approach improved results, it added complexity to the training pipeline. Additionally, previous work often considered fixed trade-offs between model size and dataset size, without systematically exploring the effect of compute budget and regularization across a wide range of scenarios.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors go beyond distillation-based techniques and offer a more comprehensive investigation into the \textbf{interplay between model size, data size, and compute budget}. They conduct experiments across different ViT variants (from small to large models) and different dataset sizes (ImageNet-1k and ImageNet-21k) while systematically adjusting the amount of compute and AugReg techniques.
	 
	 
	 This approach offers a more nuanced understanding of how to balance \textbf{model complexity, data augmentation, and regularization} to achieve optimal performance under various constraints, helping practitioners make better decisions based on their available resources.
	 
	 
	 
	 
	 
	 
	 \section{Regularization Techniques and Their Impact}
	 \subsection{Previous Works:}
	 The role of regularization in ViTs was relatively underexplored in previous work, with most efforts focusing on training larger models on massive datasets. Dropout and stochastic depth were sometimes applied, but their effects were not systematically tested across different model sizes and dataset conditions.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors explore the use of \textbf{regularization techniques} like dropout and stochastic depth in greater detail. They find that regularization primarily benefits larger models when trained for extended periods, and actually harms performance in smaller models or when training on smaller datasets like ImageNet-21k. They also conduct ablation studies to identify the best settings for regularization, determining that a peak dropout/stochastic depth probability of 0.1 works best.
	 
	 
	 
	 This more detailed exploration of regularization sets their work apart by offering actionable insights into when and how to apply regularization effectively in ViTs, providing a deeper understanding of its benefits and drawbacks.
	 
	 
	 
	 
	 
	 
	 
	 
	 \section{Impact of Model Size}
	 Larger models tend to benefit more from regularization techniques, but this comes with the cost of requiring more training time and computational resources. Smaller models, on the other hand, might not benefit as much from regularization and could even suffer a loss in performance.
	 
	 \subsection{Previous Works:}
	 Earlier studies on ViTs often treated model size as a static factor, with larger models generally preferred when training on large datasets. However, there was little guidance on how to adapt model size based on the available compute or dataset size.
	 
	 
	 \subsection{Proposed Changes:}
	 The authors provide specific \textbf{model size recommendations} based on their findings. They suggest that \textbf{larger patch sizes (e.g., 32×32)} are often more effective than reducing model size (e.g., using "Tiny" ViT models) when compute resources are limited. This means that instead of making the model smaller, increasing the patch size can help maintain performance without increasing the computational load significantly.
	 
	 This recommendation is based on a systematic analysis of throughput, model size, and patch size, providing a practical guide for selecting the right model configuration depending on the computational constraints.
	
	
	
	
	
	
	
	
	
	
	\section{Pre-training and Transfer Learning}
	The study finds that models pre-trained on larger datasets, like ImageNet-21k, perform better across a variety of tasks, including transfer learning. However, practitioners are advised to carefully choose augmentation and regularization settings to match their available compute budget and dataset size.
	
	\subsection{Previous Works:}
	Earlier ViT papers focused heavily on large-scale pre-training (e.g., JFT-300M) and emphasized the importance of pre-training on massive datasets for transfer learning. However, this left smaller organizations or researchers without access to such data at a disadvantage.
	
	
	\subsection{Proposed Changes:}
	The authors show that \textbf{transfer learning from pre-trained models on smaller datasets (like ImageNet-1k or ImageNet-21k)}, combined with strong AugReg, can yield results similar to those obtained from pre-training on massive datasets. They provide practical recommendations for selecting pre-trained models and fine-tuning them on specific tasks, which is particularly helpful for practitioners without access to large pre-training datasets.
	
	
	This change democratizes the use of ViTs, making them more accessible to a broader range of users and use cases, and helping users achieve competitive performance without requiring access to extremely large datasets for pre-training.
	
	
	\section{Practical Recommendations}
	The authors offer several practical guidelines, such as preferring data augmentation over regularization for smaller datasets, and choosing larger models with proper augmentation for the best results in a transfer learning setup.
	
	
	
	
	\section{Overall Impact of Changes}
	These changes collectively offer a more flexible and practical approach to training Vision Transformers, making them more applicable to real-world scenarios with constrained resources. The authors provide a comprehensive guide on how to balance data augmentation, regularization, and compute budget, allowing practitioners to achieve top-tier performance without relying on enormous datasets or computational resources. This is a significant shift from earlier ViT models, which focused primarily on large-scale data and heavy compute environments.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Training data-efficient image transformers \& distillation through attention \cite{DBLP:journals/corr/abs-2012-12877}}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\newpage
	\bibliographystyle{IEEEtran}
	\bibliography{refs} % Assuming your bibliography file is named bib.bib
	
\end{document}
