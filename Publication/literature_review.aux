\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{le2023binaryvit}
\citation{DBLP:journals/corr/abs-2112-13492}
\HyPL@Entry{1<</S/D>>}
\citation{DBLP:journals/corr/abs-2106-10270}
\citation{DBLP:journals/corr/abs-2012-12877}
\citation{DBLP:journals/corr/abs-2103-17239}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\citation{DBLP:journals/corr/abs-2102-11126}
\citation{DBLP:journals/corr/abs-2104-08500}
\citation{zhu2024scalablematmulfreelanguagemodeling}
\citation{zhu2024spikegptgenerativepretrainedlanguage}
\citation{DBLP:journals/corr/abs-2104-01353}
\citation{DBLP:journals/corr/abs-1812-08685}
\citation{maass1997networks}
\citation{DBLP:journals/corr/abs-2004-07532}
\citation{DBLP:journals/corr/abs-1811-00661}
\citation{thing2023deepfakedetectiondeeplearning}
\citation{Zhang_2016}
\citation{huang2024billmpushinglimitposttraining}
\citation{li2024repquantaccurateposttrainingquantization}
\citation{DBLP:journals/corr/abs-2106-14156}
\citation{ma2024era1bitllmslarge}
\citation{zhang2019rootmeansquarelayer}
\citation{wang2023bitnetscaling1bittransformers}
\citation{le2023binaryvit}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}BinaryViT \cite  {le2023binaryvit}}{8}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{8}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Proposed Model}{8}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Global Average Pooling Layer}{8}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Multiple Pooling Branches}{9}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Affine Transformation Before Residual Connections}{9}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Pyramid Structure}{9}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Binary Fully-Connected Layers with Enhanced Attention}{10}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Distillation from Full-Precision Models}{10}{subsection.1.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Impact of the Changes}{10}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Results and Improvements}{11}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Performance Improvement on ImageNet-1k}{11}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Efficiency in Terms of Operations and Parameters}{11}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Comparisons with State-of-the-Art (SOTA) Binary Models}{12}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Impact of Architectural Enhancements}{12}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Reduction in Computational Complexity}{12}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison Between Full-Precision and Binary Versions}{13}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Overall Improvements}{13}{section.1.5}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2112-13492}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Vision Transformer for Small-Size Datasets \cite  {DBLP:journals/corr/abs-2112-13492}}{14}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Shifted Patch Tokenization (SPT)}{14}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Previous Approach:}{14}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Proposed Change:}{15}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Locality Self-Attention (LSA)}{15}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Previous Approach:}{15}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Proposed Change:}{15}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Comparison to Other Data-Efficient ViTs}{16}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Efficiency vs. Performance Trade-offs}{17}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Previous Models:}{17}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Proposed Model:}{17}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Performance Gains}{17}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Overall Impact of the Proposed Changes}{17}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Results and Improvements}{18}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Performance Improvements on Small Datasets}{18}{subsection.2.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Improvements in ImageNet Performance}{18}{subsection.2.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Efficiency and Computational Overhead}{18}{subsection.2.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Ablation Study Results}{19}{subsection.2.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Qualitative Improvements}{19}{subsection.2.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Comparison with State-of-the-Art (SOTA) Models}{19}{subsection.2.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Key Takeaways:}{20}{section.2.8}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2106-10270}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers \cite  {DBLP:journals/corr/abs-2106-10270}}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data Augmentation and Regularization ("AugReg")}{21}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Previous Works:}{21}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Proposed Changes}{22}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Trade-offs Between Data, Augmentation, and Compute Budget}{22}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Previous Works:}{22}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Proposed Changes:}{22}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Regularization Techniques and Their Impact}{23}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Previous Works:}{23}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Proposed Changes:}{23}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Impact of Model Size}{23}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Previous Works:}{23}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Proposed Changes:}{23}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Pre-training and Transfer Learning}{24}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Previous Works:}{24}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Proposed Changes:}{24}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Practical Recommendations}{24}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Overall Impact of Changes}{24}{section.3.7}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2012-12877}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Training data-efficient image transformers \& distillation through attention \cite  {DBLP:journals/corr/abs-2012-12877}}{26}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Data-Efficient Image Transformers (DeiT)}{26}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Previous Works:}{26}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Proposed Changes:}{27}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Distillation Through Attention}{27}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Previous Works:}{27}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Proposed Changes:}{28}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Smaller and More Efficient Models (DeiT-S and DeiT-Ti)}{28}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Previous Works:}{28}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Proposed Changes:}{28}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Performance and Efficiency Gains}{29}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Previous Works:}{29}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Proposed Changes:}{29}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Transfer Learning and Generalization}{29}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Previous Works:}{30}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Proposed Changes:}{30}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Results and Improvements}{30}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Competitive Performance with Smaller Datasets}{30}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Distillation Through Attention Enhances Model Performance}{31}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Improved Throughput and Computational Efficiency}{31}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Smaller Models with Comparable Accuracy}{31}{subsection.4.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.5}Transfer Learning and Generalization}{32}{subsection.4.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.6}Training Time Reduction}{32}{subsection.4.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.7}Distillation from CNNs Is More Effective than from Transformers}{32}{subsection.4.6.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Overall Improvements}{33}{section.4.7}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2103-17239}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Going deeper with Image Transformers \cite  {DBLP:journals/corr/abs-2103-17239}}{34}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Key Ideas:}{34}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Deeper Vision Transformers (ViTs):}{34}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Class-Attention Mechanism:}{34}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Distillation with Class-Attention:}{34}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Efficient Training and Generalization:}{35}{subsection.5.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Performance on Benchmarks:}{35}{subsection.5.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Results and Improvements}{35}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Performance Improvement with Depth}{35}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Introduction of Class-Attention Layers}{35}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Hard-Label Distillation for Faster Convergence}{36}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Training Efficiency and Generalization}{36}{subsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Benchmark Results and Competitive Performance}{36}{subsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}Improved Attention Mechanism for Class Prediction}{37}{subsection.5.2.6}\protected@file@percent }
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Attention is All you need \cite  {DBLP:journals/corr/VaswaniSPUJGKP17}}{38}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Transformer Architecture:}{38}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Self-Attention and Multi-Head Attention:}{38}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Positional Encoding:}{39}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Advantages of Transformers:}{39}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results:}{39}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Summary}{39}{section.6.6}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2102-11126}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Deepfake Video Detection Using Convolutional Vision Transformer \cite  {DBLP:journals/corr/abs-2102-11126}}{40}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Key Components of the Model}{40}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Feature Learning through CNNs:}{40}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Global Feature Understanding through ViTs:}{40}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Comprehensive Data Preprocessing:}{41}{subsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}Testing and Results:}{41}{subsection.7.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Results and Improvements}{41}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}High Accuracy in Deepfake Detection}{41}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}AUC and Loss Metrics}{41}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Combination of CNN and ViT for Local and Global Feature Learning}{41}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Generalized Model for Different Deepfake Scenarios}{42}{subsection.7.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.5}Comparison with Other Models}{42}{subsection.7.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.6}Data Preprocessing and Face Extraction}{42}{subsection.7.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.7}Future Improvements and Expansion}{43}{subsection.7.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Summary of Improvements:}{43}{section.7.3}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2104-08500}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Visual Transformer Pruning \cite  {DBLP:journals/corr/abs-2104-08500}}{44}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Key Components of the Approach}{44}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Results and Improvements}{45}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Significant Reduction in Parameters and Computation Costs}{45}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Maintaining High Accuracy with Minimal Loss}{45}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Effectiveness on Large Datasets}{46}{subsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Flexibility of Pruning Rates}{46}{subsection.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.5}Simplicity and Efficiency of the Pruning Process}{46}{subsection.8.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.6}Promising Future Improvements}{47}{subsection.8.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Overall Improvements:}{47}{section.8.3}\protected@file@percent }
\citation{zhu2024scalablematmulfreelanguagemodeling}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Scalable MatMul-free Language Modeling \cite  {zhu2024scalablematmulfreelanguagemodeling}}{48}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Key Contributions}{48}{section.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Bitlinear Layers:}{48}{subsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Ternary Weights:}{48}{subsection.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Scalability and Efficiency:}{49}{subsection.9.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Experimental Results:}{49}{subsection.9.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Results and Improvements}{49}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Reduction in Computational Complexity}{49}{subsection.9.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Memory Efficiency}{49}{subsection.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Comparable Performance with Traditional Models}{49}{subsection.9.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Faster Inference Times}{50}{subsection.9.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.5}Scalability Across Model Sizes}{50}{subsection.9.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.6}Potential for Further Optimization Through Quantization}{50}{subsection.9.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.7}Wider Applicability Beyond Language Modeling}{51}{subsection.9.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Overall Improvements:}{51}{section.9.3}\protected@file@percent }
\citation{zhu2024spikegptgenerativepretrainedlanguage}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks \cite  {zhu2024spikegptgenerativepretrainedlanguage}}{52}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{DBLP:journals/corr/abs-2104-01353}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Deepfake Detection Scheme Based on Vision Transformer and Distillation \cite  {DBLP:journals/corr/abs-2104-01353}}{53}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Main Components}{53}{section.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Vision Transformer and EfficientNet Combination:}{53}{subsection.11.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.2}Distillation Token:}{54}{subsection.11.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.3}Performance Comparison:}{54}{subsection.11.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Results and Improvements}{54}{section.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Higher Accuracy and Better Performance Metrics}{54}{subsection.11.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}Improved Deepfake Detection Robustness}{54}{subsection.11.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Reduction in False Negatives (Improved Detection of Fake Videos)}{55}{subsection.11.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.4}Distillation Token for Better Generalization}{55}{subsection.11.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.5}Clearer Prediction of Fake Videos}{55}{subsection.11.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.6}Better Loss Reduction in Training}{56}{subsection.11.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Overall Improvements:}{56}{section.11.3}\protected@file@percent }
\citation{DBLP:journals/corr/abs-1812-08685}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}DeepFakes: a New Threat to Face Recognition? Assessment and Detection \cite  {DBLP:journals/corr/abs-1812-08685}}{57}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Deepfake Video Generation:}{57}{section.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Vulnerability of Face Recognition Systems:}{57}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Deepfake Detection Methods:}{58}{section.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Challenges for Detection Systems:}{58}{section.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Results and Improvements}{58}{section.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.1}Vulnerability of Face Recognition Systems}{58}{subsection.12.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.2}Creation of a Public Deepfake Database}{58}{subsection.12.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.3}Deepfake Detection Methods}{59}{subsection.12.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.4}Performance on Low-Quality vs. High-Quality Deepfakes}{59}{subsection.12.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.5}Improvements for Future Detection Systems}{59}{subsection.12.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Overall Results and Improvements:}{60}{section.12.6}\protected@file@percent }
\citation{maass1997networks}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Networks of spiking neurons: The third generation of neural network models \cite  {maass1997networks}}{61}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{DBLP:journals/corr/abs-2004-07532}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}DeepFakes Evolution: Analysis of Facial Regions and Fake Detection Performance \cite  {DBLP:journals/corr/abs-2004-07532}}{62}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Key Idea}{62}{section.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Key Contributions}{62}{section.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Results and Improvements}{63}{section.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Improved Detection Using Facial Region Analysis}{63}{subsection.14.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Comparison of First and Second Generation Deepfakes}{63}{subsection.14.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}Importance of Facial Artifacts in Detection}{64}{subsection.14.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.4}Challenges with High-Quality Deepfakes}{64}{subsection.14.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.5}Benchmarking and Dataset Evaluation}{64}{subsection.14.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Overall Results and Improvements}{64}{section.14.4}\protected@file@percent }
\citation{DBLP:journals/corr/abs-1811-00661}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Exposing Deep Fakes Using Inconsistent Head Poses \cite  {DBLP:journals/corr/abs-1811-00661}}{66}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Method}{66}{section.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Results and Improvements}{67}{section.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.1}Effective Detection of Deepfakes Using Head Pose Inconsistencies}{67}{subsection.15.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.2}High Detection Accuracy}{67}{subsection.15.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.3}Simplicity and Efficiency}{67}{subsection.15.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.4}Applicability Across Different Types of Deepfakes}{68}{subsection.15.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.5}Potential for Integration with Other Detection Techniques}{68}{subsection.15.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Overall Results and Improvements}{68}{section.15.3}\protected@file@percent }
\citation{thing2023deepfakedetectiondeeplearning}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers \cite  {thing2023deepfakedetectiondeeplearning}}{70}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Key Ideas}{70}{section.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Key Findings:}{71}{section.16.2}\protected@file@percent }
\citation{Zhang_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks \cite  {Zhang_2016}}{72}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Main Idea}{72}{section.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Main Contributions}{73}{section.17.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Results and Improvements}{73}{section.17.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}Improved Accuracy on Challenging Benchmarks}{73}{subsection.17.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.2}Joint Face Detection and Alignment Improves Both Tasks}{73}{subsection.17.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.3}Efficiency and Real-Time Performance}{74}{subsection.17.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.4}Online Hard Sample Mining}{74}{subsection.17.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.5}Lightweight CNN Architecture}{74}{subsection.17.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.6}Versatility Across Multiple Datasets}{75}{subsection.17.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Overall Results and Improvements}{75}{section.17.4}\protected@file@percent }
\citation{huang2024billmpushinglimitposttraining}
\@writefile{toc}{\contentsline {chapter}{\numberline {18}BiLLM: Pushing the Limit of Post-Training Quantization for LLMs \cite  {huang2024billmpushinglimitposttraining}}{76}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}Main Contributions}{76}{section.18.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {18.2}Results and Improvements}{77}{section.18.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.1}Significant Model Size Reduction}{77}{subsection.18.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.2}Minimal Accuracy Loss}{77}{subsection.18.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.3}Wide Applicability Across Model Architectures}{77}{subsection.18.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.4}Improved Computational Efficiency}{78}{subsection.18.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.5}Post-Training Quantization Simplicity}{78}{subsection.18.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.6}Potential for Future Extensions}{78}{subsection.18.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {18.3}Overall Results and Improvements}{79}{section.18.3}\protected@file@percent }
\citation{li2024repquantaccurateposttrainingquantization}
\@writefile{toc}{\contentsline {chapter}{\numberline {19}RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization \cite  {li2024repquantaccurateposttrainingquantization}}{80}{chapter.19}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {19.1}Main Contributions}{80}{section.19.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {19.2}Results and Improvements}{81}{section.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.1}Improved Accuracy in Post-Training Quantization}{81}{subsection.19.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.2}Scale Reparameterization Technique}{81}{subsection.19.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.3}Versatility Across Large Transformer Models}{82}{subsection.19.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.4}Significant Reduction in Computational Complexity}{82}{subsection.19.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.5}Hardware Compatibility and Efficiency}{82}{subsection.19.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.6}Potential for Future Extensions}{83}{subsection.19.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.7}Overall Results and Improvements}{83}{subsection.19.2.7}\protected@file@percent }
\citation{DBLP:journals/corr/abs-2106-14156}
\@writefile{toc}{\contentsline {chapter}{\numberline {20}Post-Training Quantization for Vision Transformer \cite  {DBLP:journals/corr/abs-2106-14156}}{84}{chapter.20}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {20.1}Results and Improvements}{84}{section.20.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.1}Mixed-Precision Quantization:}{84}{subsection.20.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.2}Higher Accuracy on Quantized Models:}{85}{subsection.20.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.3}Improved Performance on Smaller Models}{85}{subsection.20.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.4}Bias Correction for Error Reduction:}{85}{subsection.20.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.5}Ranking Loss for Attention Preservation:}{85}{subsection.20.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.6}Memory and Computation Savings:}{85}{subsection.20.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.7}Generalization to Object Detection:}{86}{subsection.20.1.7}\protected@file@percent }
\citation{ma2024era1bitllmslarge}
\@writefile{toc}{\contentsline {chapter}{\numberline {21}The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits \cite  {ma2024era1bitllmslarge}}{87}{chapter.21}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {21.1}Results and Improvements}{88}{section.21.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.1}Reduction in Memory Usage and Latency:}{88}{subsection.21.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.2}Energy Efficiency:}{88}{subsection.21.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.3}Throughput Improvement:}{88}{subsection.21.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.4}Maintained Accuracy:}{88}{subsection.21.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.5}Performance on Zero-Shot Tasks:}{89}{subsection.21.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.6}New Scaling Laws:}{89}{subsection.21.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.7}Hardware Optimization Potential:}{89}{subsection.21.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {21.2}Overall Improvements}{89}{section.21.2}\protected@file@percent }
\citation{zhang2019rootmeansquarelayer}
\@writefile{toc}{\contentsline {chapter}{\numberline {22}Root Mean Square Layer Normalization \cite  {zhang2019rootmeansquarelayer}}{90}{chapter.22}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {22.1}Results and Improvements}{91}{section.22.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.1}Reduction in Computational Overhead:}{91}{subsection.22.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.2}Comparable Accuracy to LayerNorm:}{91}{subsection.22.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.3}Efficiency in Machine Translation:}{91}{subsection.22.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.4}Partial RMSNorm (pRMSNorm):}{91}{subsection.22.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.5}Applicability Across Different Architectures:}{91}{subsection.22.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.6}Robustness and Stability:}{92}{subsection.22.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.7}Lower Energy and Memory Costs:}{92}{subsection.22.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {22.2}Overall Improvements:}{92}{section.22.2}\protected@file@percent }
\citation{wang2023bitnetscaling1bittransformers}
\@writefile{toc}{\contentsline {chapter}{\numberline {23}BitNet: Scaling 1-bit Transformers for Large Language Models \cite  {wang2023bitnetscaling1bittransformers}}{93}{chapter.23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {23.1}Main Contributions}{93}{section.23.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.1}Reduced Energy and Memory Costs:}{93}{subsection.23.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.2}Competitive Performance:}{94}{subsection.23.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.3}Efficient Scaling:}{94}{subsection.23.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.4}Stability in Training:}{94}{subsection.23.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.5}Group Quantization:}{94}{subsection.23.1.5}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{refs}
\bibcite{le2023binaryvit}{1}
\bibcite{DBLP:journals/corr/abs-2112-13492}{2}
\bibcite{DBLP:journals/corr/abs-2106-10270}{3}
\bibcite{DBLP:journals/corr/abs-2012-12877}{4}
\bibcite{DBLP:journals/corr/abs-2103-17239}{5}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{6}
\bibcite{DBLP:journals/corr/abs-2102-11126}{7}
\bibcite{DBLP:journals/corr/abs-2104-08500}{8}
\bibcite{zhu2024scalablematmulfreelanguagemodeling}{9}
\bibcite{zhu2024spikegptgenerativepretrainedlanguage}{10}
\bibcite{DBLP:journals/corr/abs-2104-01353}{11}
\bibcite{DBLP:journals/corr/abs-1812-08685}{12}
\bibcite{maass1997networks}{13}
\bibcite{DBLP:journals/corr/abs-2004-07532}{14}
\bibcite{DBLP:journals/corr/abs-1811-00661}{15}
\bibcite{thing2023deepfakedetectiondeeplearning}{16}
\bibcite{Zhang_2016}{17}
\bibcite{huang2024billmpushinglimitposttraining}{18}
\bibcite{li2024repquantaccurateposttrainingquantization}{19}
\bibcite{DBLP:journals/corr/abs-2106-14156}{20}
\bibcite{ma2024era1bitllmslarge}{21}
\bibcite{zhang2019rootmeansquarelayer}{22}
\bibcite{wang2023bitnetscaling1bittransformers}{23}
\gdef \@abspage@last{98}
