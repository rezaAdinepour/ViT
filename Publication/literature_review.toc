\contentsline {chapter}{\numberline {1}BinaryViT \cite {le2023binaryvit}}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}%
\contentsline {section}{\numberline {1.2}Proposed Model}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Global Average Pooling Layer}{5}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Multiple Pooling Branches}{6}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Affine Transformation Before Residual Connections}{6}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Pyramid Structure}{6}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Binary Fully-Connected Layers with Enhanced Attention}{7}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Distillation from Full-Precision Models}{7}{subsection.1.2.6}%
\contentsline {section}{\numberline {1.3}Impact of the Changes}{7}{section.1.3}%
\contentsline {section}{\numberline {1.4}Results and Improvements}{8}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Performance Improvement on ImageNet-1k}{8}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Efficiency in Terms of Operations and Parameters}{8}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Comparisons with State-of-the-Art (SOTA) Binary Models}{9}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Impact of Architectural Enhancements}{9}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}Reduction in Computational Complexity}{9}{subsection.1.4.5}%
\contentsline {subsubsection}{Comparison Between Full-Precision and Binary Versions}{10}{section*.2}%
\contentsline {section}{\numberline {1.5}Overall Improvements}{10}{section.1.5}%
\contentsline {chapter}{\numberline {2}Vision Transformer for Small-Size Datasets \cite {DBLP:journals/corr/abs-2112-13492}}{11}{chapter.2}%
\contentsline {section}{\numberline {2.1}Shifted Patch Tokenization (SPT)}{11}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Previous Approach:}{11}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Proposed Change:}{12}{subsection.2.1.2}%
\contentsline {section}{\numberline {2.2}Locality Self-Attention (LSA)}{12}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Previous Approach:}{12}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Proposed Change:}{12}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Comparison to Other Data-Efficient ViTs}{13}{section.2.3}%
\contentsline {section}{\numberline {2.4}Efficiency vs. Performance Trade-offs}{14}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Previous Models:}{14}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Proposed Model:}{14}{subsection.2.4.2}%
\contentsline {section}{\numberline {2.5}Performance Gains}{14}{section.2.5}%
\contentsline {section}{\numberline {2.6}Overall Impact of the Proposed Changes}{14}{section.2.6}%
\contentsline {section}{\numberline {2.7}Results and Improvements}{15}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Performance Improvements on Small Datasets}{15}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}Improvements in ImageNet Performance}{15}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}Efficiency and Computational Overhead}{15}{subsection.2.7.3}%
\contentsline {subsection}{\numberline {2.7.4}Ablation Study Results}{16}{subsection.2.7.4}%
\contentsline {subsection}{\numberline {2.7.5}Qualitative Improvements}{16}{subsection.2.7.5}%
\contentsline {subsection}{\numberline {2.7.6}Comparison with State-of-the-Art (SOTA) Models}{16}{subsection.2.7.6}%
\contentsline {section}{\numberline {2.8}Key Takeaways:}{17}{section.2.8}%
\contentsline {chapter}{\numberline {3}How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers \cite {DBLP:journals/corr/abs-2106-10270}}{18}{chapter.3}%
\contentsline {section}{\numberline {3.1}Data Augmentation and Regularization ("AugReg")}{18}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Previous Works:}{18}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Proposed Changes}{19}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Trade-offs Between Data, Augmentation, and Compute Budget}{19}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Previous Works:}{19}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Proposed Changes:}{19}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Regularization Techniques and Their Impact}{20}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Previous Works:}{20}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Proposed Changes:}{20}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Impact of Model Size}{20}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Previous Works:}{20}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Proposed Changes:}{20}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}Pre-training and Transfer Learning}{21}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Previous Works:}{21}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Proposed Changes:}{21}{subsection.3.5.2}%
\contentsline {section}{\numberline {3.6}Practical Recommendations}{21}{section.3.6}%
\contentsline {section}{\numberline {3.7}Overall Impact of Changes}{21}{section.3.7}%
\contentsline {chapter}{\numberline {4}Training data-efficient image transformers \& distillation through attention \cite {DBLP:journals/corr/abs-2012-12877}}{23}{chapter.4}%
\contentsline {section}{\numberline {4.1}Data-Efficient Image Transformers (DeiT)}{23}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Previous Works:}{23}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Proposed Changes:}{24}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Distillation Through Attention}{24}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Previous Works:}{24}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Proposed Changes:}{25}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}Smaller and More Efficient Models (DeiT-S and DeiT-Ti)}{25}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Previous Works:}{25}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Proposed Changes:}{25}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Performance and Efficiency Gains}{26}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Previous Works:}{26}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Proposed Changes:}{26}{subsection.4.4.2}%
\contentsline {section}{\numberline {4.5}Transfer Learning and Generalization}{26}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Previous Works:}{27}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Proposed Changes:}{27}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}Results and Improvements}{27}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Competitive Performance with Smaller Datasets}{27}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Distillation Through Attention Enhances Model Performance}{28}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}Improved Throughput and Computational Efficiency}{28}{subsection.4.6.3}%
\contentsline {subsection}{\numberline {4.6.4}Smaller Models with Comparable Accuracy}{28}{subsection.4.6.4}%
\contentsline {subsection}{\numberline {4.6.5}Transfer Learning and Generalization}{29}{subsection.4.6.5}%
\contentsline {subsection}{\numberline {4.6.6}Training Time Reduction}{29}{subsection.4.6.6}%
\contentsline {subsection}{\numberline {4.6.7}Distillation from CNNs Is More Effective than from Transformers}{29}{subsection.4.6.7}%
\contentsline {section}{\numberline {4.7}Overall Improvements}{30}{section.4.7}%
\contentsline {chapter}{\numberline {5}Going deeper with Image Transformers \cite {DBLP:journals/corr/abs-2103-17239}}{31}{chapter.5}%
\contentsline {section}{\numberline {5.1}Key Ideas:}{31}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Deeper Vision Transformers (ViTs):}{31}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Class-Attention Mechanism:}{31}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Distillation with Class-Attention:}{31}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Efficient Training and Generalization:}{32}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Performance on Benchmarks:}{32}{subsection.5.1.5}%
\contentsline {section}{\numberline {5.2}Results and Improvements}{32}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Performance Improvement with Depth}{32}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Introduction of Class-Attention Layers}{32}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Hard-Label Distillation for Faster Convergence}{33}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4}Training Efficiency and Generalization}{33}{subsection.5.2.4}%
\contentsline {subsection}{\numberline {5.2.5}Benchmark Results and Competitive Performance}{33}{subsection.5.2.5}%
\contentsline {subsection}{\numberline {5.2.6}Improved Attention Mechanism for Class Prediction}{34}{subsection.5.2.6}%
\contentsline {chapter}{\numberline {6}Attention is All you need \cite {DBLP:journals/corr/VaswaniSPUJGKP17}}{35}{chapter.6}%
\contentsline {section}{\numberline {6.1}Transformer Architecture:}{35}{section.6.1}%
\contentsline {section}{\numberline {6.2}Self-Attention and Multi-Head Attention:}{35}{section.6.2}%
\contentsline {section}{\numberline {6.3}Positional Encoding:}{36}{section.6.3}%
\contentsline {section}{\numberline {6.4}Advantages of Transformers:}{36}{section.6.4}%
\contentsline {section}{\numberline {6.5}Results:}{36}{section.6.5}%
\contentsline {section}{\numberline {6.6}Summary}{36}{section.6.6}%
\contentsline {chapter}{\numberline {7}Deepfake Video Detection Using Convolutional Vision Transformer \cite {DBLP:journals/corr/abs-2102-11126}}{37}{chapter.7}%
\contentsline {section}{\numberline {7.1}Key Components of the Model}{37}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Feature Learning through CNNs:}{37}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Global Feature Understanding through ViTs:}{37}{subsection.7.1.2}%
\contentsline {subsection}{\numberline {7.1.3}Comprehensive Data Preprocessing:}{38}{subsection.7.1.3}%
\contentsline {subsection}{\numberline {7.1.4}Testing and Results:}{38}{subsection.7.1.4}%
\contentsline {section}{\numberline {7.2}Results and Improvements}{38}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}High Accuracy in Deepfake Detection}{38}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}AUC and Loss Metrics}{38}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Combination of CNN and ViT for Local and Global Feature Learning}{38}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Generalized Model for Different Deepfake Scenarios}{39}{subsection.7.2.4}%
\contentsline {subsection}{\numberline {7.2.5}Comparison with Other Models}{39}{subsection.7.2.5}%
\contentsline {subsection}{\numberline {7.2.6}Data Preprocessing and Face Extraction}{39}{subsection.7.2.6}%
\contentsline {subsection}{\numberline {7.2.7}Future Improvements and Expansion}{40}{subsection.7.2.7}%
\contentsline {section}{\numberline {7.3}Summary of Improvements:}{40}{section.7.3}%
\contentsline {chapter}{\numberline {8}Visual Transformer Pruning \cite {DBLP:journals/corr/abs-2104-08500}}{41}{chapter.8}%
\contentsline {section}{\numberline {8.1}Key Components of the Approach}{41}{section.8.1}%
\contentsline {section}{\numberline {8.2}Results and Improvements}{42}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Significant Reduction in Parameters and Computation Costs}{42}{subsection.8.2.1}%
\contentsline {subsection}{\numberline {8.2.2}Maintaining High Accuracy with Minimal Loss}{42}{subsection.8.2.2}%
\contentsline {subsection}{\numberline {8.2.3}Effectiveness on Large Datasets}{43}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}Flexibility of Pruning Rates}{43}{subsection.8.2.4}%
\contentsline {subsection}{\numberline {8.2.5}Simplicity and Efficiency of the Pruning Process}{43}{subsection.8.2.5}%
\contentsline {subsection}{\numberline {8.2.6}Promising Future Improvements}{44}{subsection.8.2.6}%
\contentsline {section}{\numberline {8.3}Overall Improvements:}{44}{section.8.3}%
\contentsline {chapter}{\numberline {9}Scalable MatMul-free Language Modeling \cite {zhu2024scalablematmulfreelanguagemodeling}}{45}{chapter.9}%
\contentsline {section}{\numberline {9.1}Key Contributions}{45}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Bitlinear Layers:}{45}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Ternary Weights:}{45}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Scalability and Efficiency:}{46}{subsection.9.1.3}%
\contentsline {subsection}{\numberline {9.1.4}Experimental Results:}{46}{subsection.9.1.4}%
\contentsline {section}{\numberline {9.2}Results and Improvements}{46}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Reduction in Computational Complexity}{46}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}Memory Efficiency}{46}{subsection.9.2.2}%
\contentsline {subsection}{\numberline {9.2.3}Comparable Performance with Traditional Models}{46}{subsection.9.2.3}%
\contentsline {subsection}{\numberline {9.2.4}Faster Inference Times}{47}{subsection.9.2.4}%
\contentsline {subsection}{\numberline {9.2.5}Scalability Across Model Sizes}{47}{subsection.9.2.5}%
\contentsline {subsection}{\numberline {9.2.6}Potential for Further Optimization Through Quantization}{47}{subsection.9.2.6}%
\contentsline {subsection}{\numberline {9.2.7}Wider Applicability Beyond Language Modeling}{48}{subsection.9.2.7}%
\contentsline {section}{\numberline {9.3}Overall Improvements:}{48}{section.9.3}%
