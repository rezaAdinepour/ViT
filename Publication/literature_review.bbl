% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{le2023binaryvit}
P.-H.~C. Le and X.~Li, ``Binaryvit: pushing binary vision transformers towards
  convolutional models,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2023, pp. 4664--4673.

\bibitem{DBLP:journals/corr/abs-2112-13492}
\BIBentryALTinterwordspacing
S.~H. Lee, S.~Lee, and B.~C. Song, ``Vision transformer for small-size
  datasets,'' \emph{CoRR}, vol. abs/2112.13492, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2112.13492}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2106-10270}
\BIBentryALTinterwordspacing
A.~Steiner, A.~Kolesnikov, X.~Zhai, R.~Wightman, J.~Uszkoreit, and L.~Beyer,
  ``How to train your vit? data, augmentation, and regularization in vision
  transformers,'' \emph{CoRR}, vol. abs/2106.10270, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2106.10270}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2012-12877}
\BIBentryALTinterwordspacing
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'{e}}gou,
  ``Training data-efficient image transformers {\&} distillation through
  attention,'' \emph{CoRR}, vol. abs/2012.12877, 2020. [Online]. Available:
  \url{https://arxiv.org/abs/2012.12877}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2103-17239}
\BIBentryALTinterwordspacing
H.~Touvron, M.~Cord, A.~Sablayrolles, G.~Synnaeve, and H.~J{\'{e}}gou, ``Going
  deeper with image transformers,'' \emph{CoRR}, vol. abs/2103.17239, 2021.
  [Online]. Available: \url{https://arxiv.org/abs/2103.17239}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/VaswaniSPUJGKP17}
\BIBentryALTinterwordspacing
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{CoRR},
  vol. abs/1706.03762, 2017. [Online]. Available:
  \url{http://arxiv.org/abs/1706.03762}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2102-11126}
\BIBentryALTinterwordspacing
D.~Wodajo and S.~Atnafu, ``Deepfake video detection using convolutional vision
  transformer,'' \emph{CoRR}, vol. abs/2102.11126, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2102.11126}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2104-08500}
\BIBentryALTinterwordspacing
M.~Zhu, K.~Han, Y.~Tang, and Y.~Wang, ``Visual transformer pruning,''
  \emph{CoRR}, vol. abs/2104.08500, 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2104.08500}
\BIBentrySTDinterwordspacing

\bibitem{zhu2024scalablematmulfreelanguagemodeling}
\BIBentryALTinterwordspacing
R.-J. Zhu, Y.~Zhang, E.~Sifferman, T.~Sheaves, Y.~Wang, D.~Richmond, P.~Zhou,
  and J.~K. Eshraghian, ``Scalable matmul-free language modeling,'' 2024.
  [Online]. Available: \url{https://arxiv.org/abs/2406.02528}
\BIBentrySTDinterwordspacing

\bibitem{zhu2024spikegptgenerativepretrainedlanguage}
\BIBentryALTinterwordspacing
R.-J. Zhu, Q.~Zhao, G.~Li, and J.~K. Eshraghian, ``Spikegpt: Generative
  pre-trained language model with spiking neural networks,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2302.13939}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2104-01353}
\BIBentryALTinterwordspacing
Y.~J. Heo, Y.~J. Choi, Y.~Lee, and B.~Kim, ``Deepfake detection scheme based on
  vision transformer and distillation,'' \emph{CoRR}, vol. abs/2104.01353,
  2021. [Online]. Available: \url{https://arxiv.org/abs/2104.01353}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-1812-08685}
\BIBentryALTinterwordspacing
P.~Korshunov and S.~Marcel, ``Deepfakes: a new threat to face recognition?
  assessment and detection,'' \emph{CoRR}, vol. abs/1812.08685, 2018. [Online].
  Available: \url{http://arxiv.org/abs/1812.08685}
\BIBentrySTDinterwordspacing

\bibitem{maass1997networks}
W.~Maass, ``Networks of spiking neurons: the third generation of neural network
  models,'' \emph{Neural networks}, vol.~10, no.~9, pp. 1659--1671, 1997.

\bibitem{DBLP:journals/corr/abs-2004-07532}
\BIBentryALTinterwordspacing
R.~Tolosana, S.~Romero{-}Tapiador, J.~Fi{\'{e}}rrez, and
  R.~Vera{-}Rodr{\'{\i}}guez, ``Deepfakes evolution: Analysis of facial regions
  and fake detection performance,'' \emph{CoRR}, vol. abs/2004.07532, 2020.
  [Online]. Available: \url{https://arxiv.org/abs/2004.07532}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-1811-00661}
\BIBentryALTinterwordspacing
X.~Yang, Y.~Li, and S.~Lyu, ``Exposing deep fakes using inconsistent head
  poses,'' \emph{CoRR}, vol. abs/1811.00661, 2018. [Online]. Available:
  \url{http://arxiv.org/abs/1811.00661}
\BIBentrySTDinterwordspacing

\bibitem{thing2023deepfakedetectiondeeplearning}
\BIBentryALTinterwordspacing
V.~L.~L. Thing, ``Deepfake detection with deep learning: Convolutional neural
  networks versus transformers,'' 2023. [Online]. Available:
  \url{https://arxiv.org/abs/2304.03698}
\BIBentrySTDinterwordspacing

\bibitem{Zhang_2016}
\BIBentryALTinterwordspacing
K.~Zhang, Z.~Zhang, Z.~Li, and Y.~Qiao, ``Joint face detection and alignment
  using multitask cascaded convolutional networks,'' \emph{IEEE Signal
  Processing Letters}, vol.~23, no.~10, p. 1499â€“1503, Oct. 2016. [Online].
  Available: \url{http://dx.doi.org/10.1109/LSP.2016.2603342}
\BIBentrySTDinterwordspacing

\bibitem{huang2024billmpushinglimitposttraining}
\BIBentryALTinterwordspacing
W.~Huang, Y.~Liu, H.~Qin, Y.~Li, S.~Zhang, X.~Liu, M.~Magno, and X.~Qi,
  ``Billm: Pushing the limit of post-training quantization for llms,'' 2024.
  [Online]. Available: \url{https://arxiv.org/abs/2402.04291}
\BIBentrySTDinterwordspacing

\bibitem{li2024repquantaccurateposttrainingquantization}
\BIBentryALTinterwordspacing
Z.~Li, X.~Liu, J.~Zhang, and Q.~Gu, ``Repquant: Towards accurate post-training
  quantization of large transformer models via scale reparameterization,''
  2024. [Online]. Available: \url{https://arxiv.org/abs/2402.05628}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2106-14156}
\BIBentryALTinterwordspacing
Z.~Liu, Y.~Wang, K.~Han, S.~Ma, and W.~Gao, ``Post-training quantization for
  vision transformer,'' \emph{CoRR}, vol. abs/2106.14156, 2021. [Online].
  Available: \url{https://arxiv.org/abs/2106.14156}
\BIBentrySTDinterwordspacing

\bibitem{ma2024era1bitllmslarge}
\BIBentryALTinterwordspacing
S.~Ma, H.~Wang, L.~Ma, L.~Wang, W.~Wang, S.~Huang, L.~Dong, R.~Wang, J.~Xue,
  and F.~Wei, ``The era of 1-bit llms: All large language models are in 1.58
  bits,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2402.17764}
\BIBentrySTDinterwordspacing

\bibitem{zhang2019rootmeansquarelayer}
\BIBentryALTinterwordspacing
B.~Zhang and R.~Sennrich, ``Root mean square layer normalization,'' 2019.
  [Online]. Available: \url{https://arxiv.org/abs/1910.07467}
\BIBentrySTDinterwordspacing

\bibitem{wang2023bitnetscaling1bittransformers}
\BIBentryALTinterwordspacing
H.~Wang, S.~Ma, L.~Dong, S.~Huang, H.~Wang, L.~Ma, F.~Yang, R.~Wang, Y.~Wu, and
  F.~Wei, ``Bitnet: Scaling 1-bit transformers for large language models,''
  2023. [Online]. Available: \url{https://arxiv.org/abs/2310.11453}
\BIBentrySTDinterwordspacing

\end{thebibliography}
